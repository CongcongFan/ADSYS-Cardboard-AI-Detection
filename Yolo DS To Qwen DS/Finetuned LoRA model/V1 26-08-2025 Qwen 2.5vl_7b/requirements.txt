# Requirements for Fine-tuned Qwen2.5-VL LoRA to Ollama Setup
# Install with: pip install -r requirements.txt

# Core ML libraries
torch>=2.0.0
transformers>=4.37.0
accelerate>=0.25.0
peft>=0.8.0
bitsandbytes>=0.41.0

# Hugging Face libraries
huggingface_hub>=0.20.0
datasets>=2.14.0
tokenizers>=0.15.0

# Image processing (for vision model)
Pillow>=9.0.0
opencv-python>=4.8.0

# GGUF conversion (from llama.cpp)
gguf>=0.6.0
numpy>=1.24.0
sentencepiece>=0.1.99

# Model utilities
safetensors>=0.4.0
protobuf>=3.20.0

# API and testing
requests>=2.28.0
psutil>=5.9.0

# Optional: Better performance on CUDA
# ninja>=1.10.0  # Uncomment if you have CUDA and want faster compilation
# flash-attn>=2.0.0  # Uncomment for flash attention (CUDA only)

# Development and debugging
tqdm>=4.64.0
colorama>=0.4.6  # For colored terminal output