{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cardboard Quality Control - GPU-Optimized LoRA Inference\n",
    "\n",
    "## Complete Guide to Full GPU Offloading with Fine-tuned LoRA Adapter\n",
    "\n",
    "This notebook demonstrates how to use your fine-tuned LoRA adapter with maximum GPU utilization, similar to Ollama's `num_gpu=999` functionality. It includes memory optimization, batch processing, and production-ready error handling.\n",
    "\n",
    "### Features:\n",
    "- **Full GPU Offloading**: Maximize GPU usage with various precision options\n",
    "- **Memory Monitoring**: Real-time VRAM and RAM tracking\n",
    "- **LoRA Integration**: Direct LoRA loading without merging for efficiency\n",
    "- **Batch Processing**: Handle multiple images efficiently\n",
    "- **Cardboard QC Specific**: Specialized prompts and quality assessment\n",
    "- **Production Ready**: Error handling, logging, and fallback strategies\n",
    "\n",
    "### Hardware Requirements:\n",
    "- RTX 3060 (6GB VRAM) - optimized for your setup\n",
    "- Additional system RAM for model loading\n",
    "- CUDA-compatible PyTorch installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies\n",
    "\n",
    "First, let's set up the environment and install required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Checking and installing required packages...\n",
      "📦 Installing torch>=2.0.0...\n",
      "✅ torch>=2.0.0 installed successfully\n",
      "✅ torchvision already installed\n",
      "📦 Installing transformers>=4.37.0...\n",
      "✅ transformers>=4.37.0 installed successfully\n",
      "📦 Installing peft>=0.8.0...\n",
      "✅ peft>=0.8.0 installed successfully\n",
      "📦 Installing accelerate>=0.26.0...\n",
      "✅ accelerate>=0.26.0 installed successfully\n",
      "📦 Installing pillow...\n",
      "✅ pillow installed successfully\n",
      "✅ numpy already installed\n",
      "✅ psutil already installed\n",
      "✅ tqdm already installed\n",
      "✅ matplotlib already installed\n",
      "✅ seaborn already installed\n",
      "✅ pandas already installed\n",
      "✅ requests already installed\n",
      "✅ einops already installed\n",
      "✅ safetensors already installed\n",
      "\n",
      "✨ Package installation complete!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages if not already installed\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install package if not already installed.\"\"\"\n",
    "    try:\n",
    "        __import__(package.split('==')[0].replace('-', '_'))\n",
    "        print(f\"✅ {package} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"📦 Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"✅ {package} installed successfully\")\n",
    "\n",
    "# Required packages\n",
    "required_packages = [\n",
    "    \"torch>=2.0.0\",\n",
    "    \"torchvision\",\n",
    "    \"transformers>=4.37.0\",\n",
    "    \"peft>=0.8.0\",\n",
    "    \"accelerate>=0.26.0\",\n",
    "    \"pillow\",\n",
    "    \"numpy\",\n",
    "    \"psutil\",\n",
    "    \"tqdm\",\n",
    "    \"matplotlib\",\n",
    "    \"seaborn\",\n",
    "    \"pandas\",\n",
    "    \"requests\",\n",
    "    \"einops\",\n",
    "    \"safetensors\"\n",
    "]\n",
    "\n",
    "print(\"🚀 Checking and installing required packages...\")\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        install_package(package)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Warning: Could not install {package}: {e}\")\n",
    "\n",
    "print(\"\\n✨ Package installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.qwen2_vl.modeling_qwen2_vl because of the following error (look up to see its traceback):\ncannot import name 'get_num_sms' from 'torch._inductor.utils' (c:\\Users\\76135\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_inductor\\utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\76135\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1764\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   1763\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1764\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1765\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\76135\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1331\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:935\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:999\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:488\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\76135\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\qwen2_vl\\modeling_qwen2_vl.py:43\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_rope_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ROPE_INIT_FUNCTIONS\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     45\u001b[39m     add_start_docstrings,\n\u001b[32m     46\u001b[39m     add_start_docstrings_to_model_forward,\n\u001b[32m   (...)\u001b[39m\u001b[32m     50\u001b[39m     replace_return_docstrings,\n\u001b[32m     51\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\76135\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:58\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpytorch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     49\u001b[39m     Conv1D,\n\u001b[32m     50\u001b[39m     apply_chunking_to_forward,\n\u001b[32m   (...)\u001b[39m\u001b[32m     56\u001b[39m     prune_linear_layer,\n\u001b[32m     57\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquantizers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoHfQuantizer, HfQuantizer\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquantizers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquantizers_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_module_from_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\76135\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\quantizers\\__init__.py:14\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoHfQuantizer, AutoQuantizationConfig\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HfQuantizer\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\76135\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\quantizers\\auto.py:42\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquantizer_quanto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m QuantoHfQuantizer\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquantizer_torchao\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TorchAoHfQuantizer\n\u001b[32m     45\u001b[39m AUTO_QUANTIZER_MAPPING = {\n\u001b[32m     46\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mawq\u001b[39m\u001b[33m\"\u001b[39m: AwqQuantizer,\n\u001b[32m     47\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbitsandbytes_4bit\u001b[39m\u001b[33m\"\u001b[39m: Bnb4BitHfQuantizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m     56\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtorchao\u001b[39m\u001b[33m\"\u001b[39m: TorchAoHfQuantizer,\n\u001b[32m     57\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\76135\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\quantizers\\quantizer_torchao.py:35\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torchao_available():\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchao\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquantization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m quantize_\n\u001b[32m     37\u001b[39m logger = logging.get_logger(\u001b[34m__name__\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\76135\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchao\\__init__.py:41\u001b[39m\n\u001b[32m     39\u001b[39m     logging.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSkipping import of cpp extensions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchao\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquantization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     42\u001b[39m     autoquant,\n\u001b[32m     43\u001b[39m     quantize_,\n\u001b[32m     44\u001b[39m )\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dtypes, optim, quantization, swizzle, testing\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\76135\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchao\\quantization\\__init__.py:6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchao\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkernel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     int_scaled_matmul,\n\u001b[32m      3\u001b[39m     safe_int_mm,\n\u001b[32m      4\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautoquant\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      7\u001b[39m     ALL_AUTOQUANT_CLASS_LIST,\n\u001b[32m      8\u001b[39m     DEFAULT_AUTOQUANT_CLASS_LIST,\n\u001b[32m      9\u001b[39m     DEFAULT_FLOAT_AUTOQUANT_CLASS_LIST,\n\u001b[32m     10\u001b[39m     DEFAULT_INT4_AUTOQUANT_CLASS_LIST,\n\u001b[32m     11\u001b[39m     DEFAULT_SPARSE_AUTOQUANT_CLASS_LIST,\n\u001b[32m     12\u001b[39m     GEMLITE_INT4_AUTOQUANT_CLASS_LIST,\n\u001b[32m     13\u001b[39m     OTHER_AUTOQUANT_CLASS_LIST,\n\u001b[32m     14\u001b[39m     autoquant,\n\u001b[32m     15\u001b[39m )\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mGPTQ\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     17\u001b[39m     Int4WeightOnlyGPTQQuantizer,\n\u001b[32m     18\u001b[39m     MultiTensor,\n\u001b[32m     19\u001b[39m     MultiTensorInputRecorder,\n\u001b[32m     20\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\76135\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchao\\quantization\\autoquant.py:11\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchao\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchao\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     12\u001b[39m     AffineQuantizedTensor,\n\u001b[32m     13\u001b[39m     Float8Layout,\n\u001b[32m     14\u001b[39m     MarlinSparseLayout,\n\u001b[32m     15\u001b[39m     PlainLayout,\n\u001b[32m     16\u001b[39m     SemiSparseLayout,\n\u001b[32m     17\u001b[39m     TensorCoreTiledLayout,\n\u001b[32m     18\u001b[39m )\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchao\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Layout\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\76135\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchao\\dtypes\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m affine_quantized_tensor_ops\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01maffine_quantized_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      3\u001b[39m     AffineQuantizedTensor,\n\u001b[32m      4\u001b[39m     to_affine_quantized_floatx,\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     to_affine_quantized_intx_static,\n\u001b[32m     10\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\76135\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchao\\dtypes\\affine_quantized_tensor_ops.py:38\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchao\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01muintx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcutlass_int4_packed_layout\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     33\u001b[39m     _linear_int4_act_int4_weight_cutlass_check,\n\u001b[32m     34\u001b[39m     _linear_int4_act_int4_weight_cutlass_impl,\n\u001b[32m     35\u001b[39m     _linear_int8_act_int4_weight_cutlass_check,\n\u001b[32m     36\u001b[39m     _linear_int8_act_int4_weight_cutlass_impl,\n\u001b[32m     37\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchao\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01muintx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdyn_int8_act_int4_wei_cpu_layout\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     39\u001b[39m     _linear_int8_act_int4_weight_cpu_check,\n\u001b[32m     40\u001b[39m     _linear_int8_act_int4_weight_cpu_impl,\n\u001b[32m     41\u001b[39m )\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchao\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01muintx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgemlite_layout\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     43\u001b[39m     _linear_fp_act_int4_weight_gemlite_check,\n\u001b[32m     44\u001b[39m     _linear_fp_act_int4_weight_gemlite_impl,\n\u001b[32m     45\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\76135\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchao\\dtypes\\uintx\\__init__.py:7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcutlass_int4_packed_layout\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      5\u001b[39m     CutlassInt4PackedLayout,\n\u001b[32m      6\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdyn_int8_act_int4_wei_cpu_layout\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      8\u001b[39m     Int8DynamicActInt4WeightCPULayout,\n\u001b[32m      9\u001b[39m )\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mint4_cpu_layout\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     11\u001b[39m     Int4CPULayout,\n\u001b[32m     12\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\76135\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchao\\dtypes\\uintx\\dyn_int8_act_int4_wei_cpu_layout.py:320\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;66;03m# Register the concat linear fusion pass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprototype\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx_passes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m register_da8w4_concat_linear_cpu_pass\n\u001b[32m    322\u001b[39m register_da8w4_concat_linear_cpu_pass()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\76135\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchao\\prototype\\inductor\\fx_passes\\__init__.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mda8w4_concat_linear_fusion_cpu\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m register_da8w4_concat_linear_cpu_pass\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mint8_sdpa_fusion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _int8_sdpa_init\n\u001b[32m      4\u001b[39m __all__ = [\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m_int8_sdpa_init\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mregister_da8w4_concat_linear_cpu_pass\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\76135\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchao\\prototype\\inductor\\fx_passes\\int8_sdpa_fusion.py:7\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlowering\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m lowerings \u001b[38;5;28;01mas\u001b[39;00m L\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlowering\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_fallback\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\76135\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_inductor\\lowering.py:6433\u001b[39m\n\u001b[32m   6430\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m kernel\n\u001b[32m-> \u001b[39m\u001b[32m6433\u001b[39m \u001b[43mimport_submodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6435\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m quantized_lowerings\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\76135\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\utils.py:2237\u001b[39m, in \u001b[36mimport_submodule\u001b[39m\u001b[34m(mod)\u001b[39m\n\u001b[32m   2236\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m filename.endswith(\u001b[33m\"\u001b[39m\u001b[33m.py\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m filename[\u001b[32m0\u001b[39m] != \u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2237\u001b[39m     \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmod\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfilename\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\76135\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\76135\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_inductor\\kernel\\mm_scaled_grouped.py:20\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mselect_algorithm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     15\u001b[39m     autotune_select_algorithm,\n\u001b[32m     16\u001b[39m     ExternKernelChoice,\n\u001b[32m     17\u001b[39m     realize_inputs,\n\u001b[32m     18\u001b[39m     TritonTemplate,\n\u001b[32m     19\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     21\u001b[39m     get_gpu_shared_memory,\n\u001b[32m     22\u001b[39m     get_num_sms,\n\u001b[32m     23\u001b[39m     has_free_symbols,\n\u001b[32m     24\u001b[39m     use_aten_gemm_kernels,\n\u001b[32m     25\u001b[39m )\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmm_common\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     27\u001b[39m     _is_static_problem,\n\u001b[32m     28\u001b[39m     check_supported_striding,\n\u001b[32m     29\u001b[39m     persistent_grouped_mm_grid,\n\u001b[32m     30\u001b[39m )\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'get_num_sms' from 'torch._inductor.utils' (c:\\Users\\76135\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_inductor\\utils.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m prune\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransforms\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      7\u001b[39m     Qwen2VLForConditionalGeneration,\n\u001b[32m      8\u001b[39m     Qwen2VLProcessor,\n\u001b[32m      9\u001b[39m     AutoTokenizer,\n\u001b[32m     10\u001b[39m     AutoConfig,\n\u001b[32m     11\u001b[39m     BitsAndBytesConfig\n\u001b[32m     12\u001b[39m )\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftModel, PeftConfig, get_peft_model, LoraConfig\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01maccelerate\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1412\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\76135\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1755\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1753\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module.keys():\n\u001b[32m   1754\u001b[39m     module = \u001b[38;5;28mself\u001b[39m._get_module(\u001b[38;5;28mself\u001b[39m._class_to_module[name])\n\u001b[32m-> \u001b[39m\u001b[32m1755\u001b[39m     value = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1756\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1757\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\76135\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1754\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1752\u001b[39m     value = Placeholder\n\u001b[32m   1753\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module.keys():\n\u001b[32m-> \u001b[39m\u001b[32m1754\u001b[39m     module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1755\u001b[39m     value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   1756\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\76135\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1766\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   1764\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   1765\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1766\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1767\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m because of the following error (look up to see its\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1768\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1769\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Failed to import transformers.models.qwen2_vl.modeling_qwen2_vl because of the following error (look up to see its traceback):\ncannot import name 'get_num_sms' from 'torch._inductor.utils' (c:\\Users\\76135\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_inductor\\utils.py)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import prune\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import (\n",
    "    Qwen2VLForConditionalGeneration,\n",
    "    Qwen2VLProcessor,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "import accelerate\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import psutil\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Union, Tuple, Any\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import threading\n",
    "import queue\n",
    "from dataclasses import dataclass\n",
    "from contextlib import contextmanager\n",
    "\n",
    "# Configure warnings and logging\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler('cardboard_qc_gpu.log')\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ All imports successful!\")\n",
    "print(f\"🔥 PyTorch version: {torch.__version__}\")\n",
    "print(f\"🤗 Transformers version: {transformers.__version__}\")\n",
    "print(f\"🎯 PEFT version: {peft.__version__}\")\n",
    "print(f\"⚡ Accelerate version: {accelerate.__version__}\")\n",
    "print(f\"🐍 Python version: {sys.version.split()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GPU Memory Management and Monitoring\n",
    "\n",
    "This section provides comprehensive GPU memory monitoring and optimization utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataclass' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mMemoryStats\u001b[39;00m:\n\u001b[32m      3\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Data class for memory statistics.\"\"\"\u001b[39;00m\n\u001b[32m      4\u001b[39m     ram_total_gb: \u001b[38;5;28mfloat\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'dataclass' is not defined"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class MemoryStats:\n",
    "    \"\"\"Data class for memory statistics.\"\"\"\n",
    "    ram_total_gb: float\n",
    "    ram_available_gb: float\n",
    "    ram_used_percent: float\n",
    "    gpu_total_gb: float = 0.0\n",
    "    gpu_allocated_gb: float = 0.0\n",
    "    gpu_reserved_gb: float = 0.0\n",
    "    gpu_free_gb: float = 0.0\n",
    "    gpu_utilization_percent: float = 0.0\n",
    "\n",
    "class GPUMemoryManager:\n",
    "    \"\"\"Advanced GPU memory management and monitoring.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.device_count = torch.cuda.device_count() if torch.cuda.is_available() else 0\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.memory_history = []\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.gpu_properties = torch.cuda.get_device_properties(0)\n",
    "            print(f\"🎮 GPU: {self.gpu_properties.name}\")\n",
    "            print(f\"💾 GPU Memory: {self.gpu_properties.total_memory / 1024**3:.1f} GB\")\n",
    "            print(f\"🔢 GPU Compute Capability: {self.gpu_properties.major}.{self.gpu_properties.minor}\")\n",
    "        else:\n",
    "            print(\"⚠️ No CUDA GPU available, using CPU\")\n",
    "    \n",
    "    def get_memory_stats(self) -> MemoryStats:\n",
    "        \"\"\"Get comprehensive memory statistics.\"\"\"\n",
    "        # RAM statistics\n",
    "        ram = psutil.virtual_memory()\n",
    "        ram_total_gb = ram.total / (1024**3)\n",
    "        ram_available_gb = ram.available / (1024**3)\n",
    "        ram_used_percent = ram.percent\n",
    "        \n",
    "        # GPU statistics\n",
    "        gpu_stats = MemoryStats(\n",
    "            ram_total_gb=ram_total_gb,\n",
    "            ram_available_gb=ram_available_gb,\n",
    "            ram_used_percent=ram_used_percent\n",
    "        )\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            gpu_stats.gpu_total_gb = self.gpu_properties.total_memory / (1024**3)\n",
    "            gpu_stats.gpu_allocated_gb = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "            gpu_stats.gpu_reserved_gb = torch.cuda.memory_reserved(0) / (1024**3)\n",
    "            gpu_stats.gpu_free_gb = gpu_stats.gpu_total_gb - gpu_stats.gpu_allocated_gb\n",
    "            gpu_stats.gpu_utilization_percent = (gpu_stats.gpu_allocated_gb / gpu_stats.gpu_total_gb) * 100\n",
    "        \n",
    "        return gpu_stats\n",
    "    \n",
    "    def log_memory_usage(self, context: str = \"\", save_to_history: bool = True):\n",
    "        \"\"\"Log current memory usage with optional context.\"\"\"\n",
    "        stats = self.get_memory_stats()\n",
    "        \n",
    "        print(f\"\\n📊 Memory Usage {context}:\")\n",
    "        print(f\"  🖥️  RAM: {stats.ram_used_percent:.1f}% ({stats.ram_available_gb:.1f}GB available / {stats.ram_total_gb:.1f}GB total)\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"  🎮 GPU: {stats.gpu_utilization_percent:.1f}% ({stats.gpu_allocated_gb:.2f}GB allocated / {stats.gpu_total_gb:.1f}GB total)\")\n",
    "            print(f\"  💨 GPU Free: {stats.gpu_free_gb:.2f}GB\")\n",
    "            print(f\"  📦 GPU Reserved: {stats.gpu_reserved_gb:.2f}GB\")\n",
    "        \n",
    "        if save_to_history:\n",
    "            self.memory_history.append({\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'context': context,\n",
    "                'stats': stats\n",
    "            })\n",
    "    \n",
    "    def optimize_gpu_memory(self, aggressive: bool = False):\n",
    "        \"\"\"Optimize GPU memory usage.\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return\n",
    "        \n",
    "        print(\"🧹 Optimizing GPU memory...\")\n",
    "        \n",
    "        # Standard cleanup\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "        \n",
    "        if aggressive:\n",
    "            # Aggressive cleanup\n",
    "            with torch.cuda.device(0):\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.reset_peak_memory_stats()\n",
    "        \n",
    "        print(\"✅ GPU memory optimization complete\")\n",
    "    \n",
    "    def get_optimal_batch_size(self, model_size_gb: float, safety_factor: float = 0.8) -> int:\n",
    "        \"\"\"Calculate optimal batch size based on available GPU memory.\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return 1\n",
    "        \n",
    "        stats = self.get_memory_stats()\n",
    "        available_memory = stats.gpu_free_gb * safety_factor\n",
    "        \n",
    "        # Estimate memory per sample (rough approximation)\n",
    "        memory_per_sample = 0.5  # GB - adjust based on your model and input size\n",
    "        \n",
    "        batch_size = max(1, int(available_memory / memory_per_sample))\n",
    "        print(f\"🎯 Recommended batch size: {batch_size} (Available: {available_memory:.1f}GB)\")\n",
    "        \n",
    "        return min(batch_size, 8)  # Cap at 8 for stability\n",
    "    \n",
    "    def plot_memory_history(self, save_path: Optional[str] = None):\n",
    "        \"\"\"Plot memory usage history.\"\"\"\n",
    "        if not self.memory_history:\n",
    "            print(\"No memory history to plot\")\n",
    "            return\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "        \n",
    "        # Extract data\n",
    "        timestamps = [datetime.fromisoformat(h['timestamp']) for h in self.memory_history]\n",
    "        ram_usage = [h['stats'].ram_used_percent for h in self.memory_history]\n",
    "        gpu_usage = [h['stats'].gpu_utilization_percent for h in self.memory_history if torch.cuda.is_available()]\n",
    "        \n",
    "        # Plot RAM usage\n",
    "        ax1.plot(timestamps, ram_usage, 'b-', linewidth=2, marker='o')\n",
    "        ax1.set_title('RAM Usage Over Time')\n",
    "        ax1.set_ylabel('RAM Usage (%)')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.set_ylim(0, 100)\n",
    "        \n",
    "        # Plot GPU usage\n",
    "        if torch.cuda.is_available() and gpu_usage:\n",
    "            ax2.plot(timestamps, gpu_usage, 'r-', linewidth=2, marker='s')\n",
    "            ax2.set_title('GPU Memory Usage Over Time')\n",
    "            ax2.set_ylabel('GPU Memory Usage (%)')\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            ax2.set_ylim(0, 100)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"📊 Memory usage plot saved to {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    @contextmanager\n",
    "    def memory_tracking(self, context: str):\n",
    "        \"\"\"Context manager for automatic memory tracking.\"\"\"\n",
    "        self.log_memory_usage(f\"before {context}\")\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            self.log_memory_usage(f\"after {context}\")\n",
    "\n",
    "# Initialize memory manager\n",
    "memory_manager = GPUMemoryManager()\n",
    "memory_manager.log_memory_usage(\"initialization\")\n",
    "\n",
    "print(\"\\n✅ GPU Memory Management System initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Configuration and Precision Options\n",
    "\n",
    "Configure different precision levels and GPU offloading strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelPrecisionConfig:\n",
    "    \"\"\"Configuration class for different model precision levels.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_fp16_config():\n",
    "        \"\"\"Float16 - Good balance of speed and quality.\"\"\"\n",
    "        return {\n",
    "            'torch_dtype': torch.float16,\n",
    "            'device_map': 'auto',\n",
    "            'low_cpu_mem_usage': True,\n",
    "            'trust_remote_code': True,\n",
    "            'attn_implementation': 'flash_attention_2' if hasattr(torch.nn, 'MultiheadAttention') else 'eager'\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_bf16_config():\n",
    "        \"\"\"BFloat16 - Better numerical stability than FP16.\"\"\"\n",
    "        return {\n",
    "            'torch_dtype': torch.bfloat16,\n",
    "            'device_map': 'auto',\n",
    "            'low_cpu_mem_usage': True,\n",
    "            'trust_remote_code': True,\n",
    "            'attn_implementation': 'flash_attention_2' if hasattr(torch.nn, 'MultiheadAttention') else 'eager'\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_int8_config():\n",
    "        \"\"\"8-bit quantization - Significant memory savings.\"\"\"\n",
    "        return {\n",
    "            'quantization_config': BitsAndBytesConfig(\n",
    "                load_in_8bit=True,\n",
    "                llm_int8_enable_fp32_cpu_offload=True,\n",
    "                llm_int8_has_fp16_weight=True\n",
    "            ),\n",
    "            'device_map': 'auto',\n",
    "            'low_cpu_mem_usage': True,\n",
    "            'trust_remote_code': True\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_int4_config():\n",
    "        \"\"\"4-bit quantization - Maximum memory savings.\"\"\"\n",
    "        return {\n",
    "            'quantization_config': BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\"\n",
    "            ),\n",
    "            'device_map': 'auto',\n",
    "            'low_cpu_mem_usage': True,\n",
    "            'trust_remote_code': True\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_cpu_config():\n",
    "        \"\"\"CPU-only configuration.\"\"\"\n",
    "        return {\n",
    "            'torch_dtype': torch.float32,\n",
    "            'device_map': 'cpu',\n",
    "            'low_cpu_mem_usage': True,\n",
    "            'trust_remote_code': True\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_config_by_name(name: str):\n",
    "        \"\"\"Get configuration by name.\"\"\"\n",
    "        configs = {\n",
    "            'fp16': ModelPrecisionConfig.get_fp16_config(),\n",
    "            'bf16': ModelPrecisionConfig.get_bf16_config(),\n",
    "            'int8': ModelPrecisionConfig.get_int8_config(),\n",
    "            'int4': ModelPrecisionConfig.get_int4_config(),\n",
    "            'cpu': ModelPrecisionConfig.get_cpu_config()\n",
    "        }\n",
    "        \n",
    "        if name not in configs:\n",
    "            available = list(configs.keys())\n",
    "            raise ValueError(f\"Unknown config '{name}'. Available: {available}\")\n",
    "        \n",
    "        return configs[name]\n",
    "    \n",
    "    @staticmethod\n",
    "    def recommend_config(target_memory_gb: float = 6.0) -> str:\n",
    "        \"\"\"Recommend best configuration for target memory.\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return 'cpu'\n",
    "        \n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "        \n",
    "        if gpu_memory >= 12:\n",
    "            return 'fp16'  # High-end GPU\n",
    "        elif gpu_memory >= 8:\n",
    "            return 'bf16'  # Mid-range GPU\n",
    "        elif gpu_memory >= 6:\n",
    "            return 'int8'  # Your RTX 3060\n",
    "        else:\n",
    "            return 'int4'  # Low VRAM GPU\n",
    "\n",
    "# Test precision configurations\n",
    "print(\"🔧 Available Precision Configurations:\")\n",
    "for name in ['fp16', 'bf16', 'int8', 'int4', 'cpu']:\n",
    "    try:\n",
    "        config = ModelPrecisionConfig.get_config_by_name(name)\n",
    "        print(f\"  ✅ {name.upper()}: {config.get('torch_dtype', 'N/A')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ {name.upper()}: {e}\")\n",
    "\n",
    "# Get recommendation for your RTX 3060\n",
    "recommended = ModelPrecisionConfig.recommend_config(6.0)\n",
    "print(f\"\\n🎯 Recommended configuration for RTX 3060: {recommended.upper()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced LoRA Inference Engine\n",
    "\n",
    "Production-ready LoRA inference engine with full GPU offloading and optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedLoRAInferenceEngine:\n",
    "    \"\"\"Advanced LoRA inference engine with full GPU optimization.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        base_model_path: str,\n",
    "        lora_path: str,\n",
    "        precision: str = 'int8',\n",
    "        enable_compilation: bool = True,\n",
    "        memory_manager: Optional[GPUMemoryManager] = None\n",
    "    ):\n",
    "        self.base_model_path = Path(base_model_path)\n",
    "        self.lora_path = Path(lora_path)\n",
    "        self.precision = precision\n",
    "        self.enable_compilation = enable_compilation\n",
    "        self.memory_manager = memory_manager or GPUMemoryManager()\n",
    "        \n",
    "        # Model components\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.processor = None\n",
    "        self.generation_config = None\n",
    "        \n",
    "        # State tracking\n",
    "        self.is_loaded = False\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.compiled_model = None\n",
    "        \n",
    "        # Performance tracking\n",
    "        self.inference_times = []\n",
    "        self.memory_snapshots = []\n",
    "        \n",
    "        logger.info(f\"🚀 Advanced LoRA Engine initialized\")\n",
    "        logger.info(f\"📁 Base model: {self.base_model_path}\")\n",
    "        logger.info(f\"🎯 LoRA adapter: {self.lora_path}\")\n",
    "        logger.info(f\"⚙️ Precision: {precision}\")\n",
    "        logger.info(f\"🔥 Compilation: {enable_compilation}\")\n",
    "    \n",
    "    def validate_paths(self) -> bool:\n",
    "        \"\"\"Validate that all required paths exist.\"\"\"\n",
    "        logger.info(\"🔍 Validating model paths...\")\n",
    "        \n",
    "        # Check base model\n",
    "        if not self.base_model_path.exists():\n",
    "            logger.error(f\"❌ Base model path not found: {self.base_model_path}\")\n",
    "            return False\n",
    "        \n",
    "        base_config = self.base_model_path / \"config.json\"\n",
    "        if not base_config.exists():\n",
    "            logger.error(f\"❌ Base model config not found: {base_config}\")\n",
    "            return False\n",
    "        \n",
    "        # Check LoRA adapter\n",
    "        if not self.lora_path.exists():\n",
    "            logger.error(f\"❌ LoRA path not found: {self.lora_path}\")\n",
    "            return False\n",
    "        \n",
    "        lora_config = self.lora_path / \"adapter_config.json\"\n",
    "        lora_weights = self.lora_path / \"adapter_model.safetensors\"\n",
    "        \n",
    "        if not lora_config.exists():\n",
    "            logger.error(f\"❌ LoRA config not found: {lora_config}\")\n",
    "            return False\n",
    "        \n",
    "        if not lora_weights.exists():\n",
    "            logger.error(f\"❌ LoRA weights not found: {lora_weights}\")\n",
    "            return False\n",
    "        \n",
    "        logger.info(\"✅ All paths validated successfully\")\n",
    "        return True\n",
    "    \n",
    "    def load_model(self, force_reload: bool = False):\n",
    "        \"\"\"Load model with optimal GPU configuration.\"\"\"\n",
    "        if self.is_loaded and not force_reload:\n",
    "            logger.info(\"Model already loaded\")\n",
    "            return\n",
    "        \n",
    "        if not self.validate_paths():\n",
    "            raise RuntimeError(\"Path validation failed\")\n",
    "        \n",
    "        logger.info(f\"🔄 Loading model with {self.precision} precision...\")\n",
    "        \n",
    "        with self.memory_manager.memory_tracking(\"model loading\"):\n",
    "            try:\n",
    "                # Get precision configuration\n",
    "                model_config = ModelPrecisionConfig.get_config_by_name(self.precision)\n",
    "                logger.info(f\"📋 Model config: {model_config}\")\n",
    "                \n",
    "                # Load LoRA configuration\n",
    "                logger.info(\"📖 Loading LoRA configuration...\")\n",
    "                peft_config = PeftConfig.from_pretrained(str(self.lora_path))\n",
    "                logger.info(f\"🎯 LoRA target modules: {peft_config.target_modules}\")\n",
    "                logger.info(f\"🔢 LoRA rank: {peft_config.r}\")\n",
    "                logger.info(f\"📊 LoRA alpha: {peft_config.lora_alpha}\")\n",
    "                \n",
    "                # Load base model\n",
    "                logger.info(\"🏗️ Loading base model (this may take a few minutes)...\")\n",
    "                \n",
    "                # Special handling for quantized models\n",
    "                if 'quantization_config' in model_config:\n",
    "                    # For quantized models, load without device_map first\n",
    "                    base_model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "                        str(self.base_model_path),\n",
    "                        quantization_config=model_config['quantization_config'],\n",
    "                        low_cpu_mem_usage=True,\n",
    "                        trust_remote_code=True\n",
    "                    )\n",
    "                else:\n",
    "                    # For non-quantized models\n",
    "                    base_model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "                        str(self.base_model_path),\n",
    "                        **model_config\n",
    "                    )\n",
    "                \n",
    "                logger.info(\"✅ Base model loaded successfully!\")\n",
    "                \n",
    "                # Load LoRA adapter\n",
    "                logger.info(\"🎯 Loading LoRA adapter...\")\n",
    "                self.model = PeftModel.from_pretrained(\n",
    "                    base_model,\n",
    "                    str(self.lora_path),\n",
    "                    is_trainable=False\n",
    "                )\n",
    "                \n",
    "                logger.info(\"✅ LoRA adapter loaded successfully!\")\n",
    "                \n",
    "                # Load tokenizer and processor\n",
    "                logger.info(\"📝 Loading tokenizer and processor...\")\n",
    "                \n",
    "                try:\n",
    "                    self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                        str(self.lora_path),\n",
    "                        trust_remote_code=True\n",
    "                    )\n",
    "                    self.processor = Qwen2VLProcessor.from_pretrained(\n",
    "                        str(self.lora_path)\n",
    "                    )\n",
    "                except:\n",
    "                    # Fallback to base model\n",
    "                    logger.warning(\"Using base model tokenizer/processor as fallback\")\n",
    "                    self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                        str(self.base_model_path),\n",
    "                        trust_remote_code=True\n",
    "                    )\n",
    "                    self.processor = Qwen2VLProcessor.from_pretrained(\n",
    "                        str(self.base_model_path)\n",
    "                    )\n",
    "                \n",
    "                # Set evaluation mode\n",
    "                self.model.eval()\n",
    "                \n",
    "                # Compile model for optimization (if enabled)\n",
    "                if self.enable_compilation and torch.cuda.is_available():\n",
    "                    try:\n",
    "                        logger.info(\"🔥 Compiling model for optimization...\")\n",
    "                        self.compiled_model = torch.compile(\n",
    "                            self.model,\n",
    "                            mode=\"reduce-overhead\",\n",
    "                            fullgraph=False\n",
    "                        )\n",
    "                        logger.info(\"✅ Model compilation successful!\")\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"⚠️ Model compilation failed: {e}\")\n",
    "                        self.compiled_model = None\n",
    "                \n",
    "                # Configure generation parameters\n",
    "                self.generation_config = {\n",
    "                    'max_new_tokens': 512,\n",
    "                    'do_sample': True,\n",
    "                    'temperature': 0.7,\n",
    "                    'top_p': 0.9,\n",
    "                    'top_k': 50,\n",
    "                    'repetition_penalty': 1.1,\n",
    "                    'pad_token_id': self.tokenizer.eos_token_id,\n",
    "                    'use_cache': True\n",
    "                }\n",
    "                \n",
    "                self.is_loaded = True\n",
    "                logger.info(\"🎉 Model loading complete!\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"❌ Error loading model: {e}\")\n",
    "                import traceback\n",
    "                logger.error(traceback.format_exc())\n",
    "                raise\n",
    "    \n",
    "    def unload_model(self):\n",
    "        \"\"\"Unload model and free memory.\"\"\"\n",
    "        logger.info(\"🧹 Unloading model...\")\n",
    "        \n",
    "        if self.model:\n",
    "            del self.model\n",
    "        if self.compiled_model:\n",
    "            del self.compiled_model\n",
    "        if self.tokenizer:\n",
    "            del self.tokenizer\n",
    "        if self.processor:\n",
    "            del self.processor\n",
    "        \n",
    "        self.model = None\n",
    "        self.compiled_model = None\n",
    "        self.tokenizer = None\n",
    "        self.processor = None\n",
    "        self.is_loaded = False\n",
    "        \n",
    "        # Cleanup memory\n",
    "        self.memory_manager.optimize_gpu_memory(aggressive=True)\n",
    "        \n",
    "        logger.info(\"✅ Model unloaded successfully\")\n",
    "    \n",
    "    def load_image(self, image_source: Union[str, Path, Image.Image]) -> Image.Image:\n",
    "        \"\"\"Load and preprocess image.\"\"\"\n",
    "        if isinstance(image_source, Image.Image):\n",
    "            image = image_source\n",
    "        elif isinstance(image_source, (str, Path)):\n",
    "            image_path = Path(image_source)\n",
    "            if not image_path.exists():\n",
    "                raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
    "            image = Image.open(image_path)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid image source type\")\n",
    "        \n",
    "        # Convert to RGB if needed\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        \n",
    "        return image\n",
    "    \n",
    "    def generate_response(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        image: Optional[Union[str, Path, Image.Image]] = None,\n",
    "        **generation_kwargs\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Generate response with performance tracking.\"\"\"\n",
    "        if not self.is_loaded:\n",
    "            raise RuntimeError(\"Model not loaded. Call load_model() first.\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            with self.memory_manager.memory_tracking(\"inference\"):\n",
    "                # Prepare input\n",
    "                messages = []\n",
    "                \n",
    "                if image is not None:\n",
    "                    image_obj = self.load_image(image)\n",
    "                    messages.append({\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\"type\": \"image\", \"image\": image_obj},\n",
    "                            {\"type\": \"text\", \"text\": prompt}\n",
    "                        ]\n",
    "                    })\n",
    "                else:\n",
    "                    messages.append({\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt\n",
    "                    })\n",
    "                \n",
    "                # Apply chat template\n",
    "                text = self.processor.apply_chat_template(\n",
    "                    messages,\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True\n",
    "                )\n",
    "                \n",
    "                # Process inputs\n",
    "                if image is not None:\n",
    "                    image_obj = self.load_image(image)\n",
    "                    inputs = self.processor(\n",
    "                        text=[text],\n",
    "                        images=[image_obj],\n",
    "                        return_tensors=\"pt\",\n",
    "                        padding=True\n",
    "                    )\n",
    "                else:\n",
    "                    inputs = self.processor(\n",
    "                        text=[text],\n",
    "                        return_tensors=\"pt\",\n",
    "                        padding=True\n",
    "                    )\n",
    "                \n",
    "                # Move to device\n",
    "                if torch.cuda.is_available():\n",
    "                    inputs = inputs.to(self.device)\n",
    "                \n",
    "                # Merge generation config\n",
    "                final_config = {**self.generation_config, **generation_kwargs}\n",
    "                \n",
    "                # Generate\n",
    "                model_to_use = self.compiled_model if self.compiled_model else self.model\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    generated_ids = model_to_use.generate(\n",
    "                        **inputs,\n",
    "                        **final_config\n",
    "                    )\n",
    "                \n",
    "                # Decode response\n",
    "                input_token_len = inputs.input_ids.shape[1]\n",
    "                new_tokens = generated_ids[:, input_token_len:]\n",
    "                \n",
    "                response = self.tokenizer.decode(\n",
    "                    new_tokens[0],\n",
    "                    skip_special_tokens=True\n",
    "                ).strip()\n",
    "                \n",
    "                # Performance tracking\n",
    "                end_time = time.time()\n",
    "                inference_time = end_time - start_time\n",
    "                self.inference_times.append(inference_time)\n",
    "                \n",
    "                # Token statistics\n",
    "                input_tokens = inputs.input_ids.shape[1]\n",
    "                output_tokens = new_tokens.shape[1]\n",
    "                total_tokens = input_tokens + output_tokens\n",
    "                tokens_per_second = output_tokens / inference_time\n",
    "                \n",
    "                result = {\n",
    "                    'response': response,\n",
    "                    'inference_time': inference_time,\n",
    "                    'input_tokens': input_tokens,\n",
    "                    'output_tokens': output_tokens,\n",
    "                    'total_tokens': total_tokens,\n",
    "                    'tokens_per_second': tokens_per_second,\n",
    "                    'prompt': prompt,\n",
    "                    'has_image': image is not None\n",
    "                }\n",
    "                \n",
    "                logger.info(f\"🎯 Generated {output_tokens} tokens in {inference_time:.2f}s ({tokens_per_second:.1f} tok/s)\")\n",
    "                \n",
    "                return result\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Generation error: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def get_performance_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get inference performance statistics.\"\"\"\n",
    "        if not self.inference_times:\n",
    "            return {\"message\": \"No inference data available\"}\n",
    "        \n",
    "        times = np.array(self.inference_times)\n",
    "        \n",
    "        return {\n",
    "            'total_inferences': len(times),\n",
    "            'avg_time': float(np.mean(times)),\n",
    "            'median_time': float(np.median(times)),\n",
    "            'min_time': float(np.min(times)),\n",
    "            'max_time': float(np.max(times)),\n",
    "            'std_time': float(np.std(times))\n",
    "        }\n",
    "\n",
    "print(\"✅ Advanced LoRA Inference Engine ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cardboard Quality Control Specialization\n",
    "\n",
    "Specialized prompts and processing for cardboard quality control tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CardboardQCProcessor:\n",
    "    \"\"\"Specialized processor for cardboard quality control tasks.\"\"\"\n",
    "    \n",
    "    # Quality control prompts based on your training data\n",
    "    QC_PROMPTS = {\n",
    "        'detailed_analysis': \"\"\"\n",
    "Analyze this cardboard image for quality control purposes. Please provide a detailed assessment including:\n",
    "\n",
    "1. **Overall Quality**: Pass/Fail determination\n",
    "2. **Visual Defects**: Any visible damage, warping, creases, or deformations\n",
    "3. **Surface Condition**: Scratches, stains, discoloration, or surface irregularities\n",
    "4. **Structural Issues**: Bent edges, torn sections, or compromised integrity\n",
    "5. **Confidence Score**: Your confidence in the assessment (0-100%)\n",
    "\n",
    "Provide your response in a clear, structured format suitable for quality control documentation.\n",
    "\"\"\".strip(),\n",
    "        \n",
    "        'pass_fail_only': \"\"\"\n",
    "Examine this cardboard for quality control. Determine if this cardboard should PASS or FAIL quality inspection.\n",
    "\n",
    "Respond with:\n",
    "- PASS or FAIL\n",
    "- Brief reason (1-2 sentences)\n",
    "- Confidence percentage\n",
    "\"\"\".strip(),\n",
    "        \n",
    "        'defect_detection': \"\"\"\n",
    "Look at this cardboard image and identify any defects or quality issues. Focus on:\n",
    "- Warping or bending\n",
    "- Surface damage\n",
    "- Structural problems\n",
    "- Manufacturing defects\n",
    "\n",
    "List each defect found and its severity (Minor/Major/Critical).\n",
    "\"\"\".strip(),\n",
    "        \n",
    "        'comparative_analysis': \"\"\"\n",
    "Analyze this cardboard sample and compare it to standard quality expectations for packaging materials.\n",
    "Rate the following aspects (1-10 scale):\n",
    "- Structural integrity\n",
    "- Surface quality\n",
    "- Overall condition\n",
    "- Usability for packaging\n",
    "\"\"\".strip()\n",
    "    }\n",
    "    \n",
    "    def __init__(self, inference_engine: AdvancedLoRAInferenceEngine):\n",
    "        self.engine = inference_engine\n",
    "        self.qc_history = []\n",
    "        \n",
    "    def analyze_cardboard(\n",
    "        self,\n",
    "        image_path: Union[str, Path],\n",
    "        analysis_type: str = 'detailed_analysis',\n",
    "        custom_prompt: Optional[str] = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze cardboard quality with specialized prompts.\"\"\"\n",
    "        \n",
    "        # Select prompt\n",
    "        if custom_prompt:\n",
    "            prompt = custom_prompt\n",
    "        elif analysis_type in self.QC_PROMPTS:\n",
    "            prompt = self.QC_PROMPTS[analysis_type]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown analysis type: {analysis_type}\")\n",
    "        \n",
    "        # Generate analysis\n",
    "        result = self.engine.generate_response(\n",
    "            prompt=prompt,\n",
    "            image=image_path,\n",
    "            temperature=0.3,  # Lower temperature for more consistent QC results\n",
    "            max_new_tokens=384\n",
    "        )\n",
    "        \n",
    "        # Parse QC-specific information\n",
    "        qc_result = self._parse_qc_response(result['response'])\n",
    "        \n",
    "        # Combine with generation stats\n",
    "        full_result = {\n",
    "            **result,\n",
    "            'qc_analysis': qc_result,\n",
    "            'image_path': str(image_path),\n",
    "            'analysis_type': analysis_type,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Save to history\n",
    "        self.qc_history.append(full_result)\n",
    "        \n",
    "        return full_result\n",
    "    \n",
    "    def _parse_qc_response(self, response: str) -> Dict[str, Any]:\n",
    "        \"\"\"Parse QC response to extract structured information.\"\"\"\n",
    "        qc_info = {\n",
    "            'raw_response': response,\n",
    "            'pass_fail': None,\n",
    "            'confidence': None,\n",
    "            'defects': [],\n",
    "            'severity': 'unknown'\n",
    "        }\n",
    "        \n",
    "        response_lower = response.lower()\n",
    "        \n",
    "        # Extract pass/fail decision\n",
    "        if 'pass' in response_lower and 'fail' not in response_lower:\n",
    "            qc_info['pass_fail'] = 'PASS'\n",
    "        elif 'fail' in response_lower:\n",
    "            qc_info['pass_fail'] = 'FAIL'\n",
    "        \n",
    "        # Extract confidence percentage\n",
    "        import re\n",
    "        confidence_match = re.search(r'confidence[:\\s]*([0-9]+)%?', response_lower)\n",
    "        if confidence_match:\n",
    "            qc_info['confidence'] = int(confidence_match.group(1))\n",
    "        \n",
    "        # Extract defects (simple keyword matching)\n",
    "        defect_keywords = ['warp', 'bend', 'crease', 'tear', 'damage', 'stain', 'scratch', 'dent']\n",
    "        found_defects = [kw for kw in defect_keywords if kw in response_lower]\n",
    "        qc_info['defects'] = found_defects\n",
    "        \n",
    "        # Determine severity\n",
    "        if 'critical' in response_lower:\n",
    "            qc_info['severity'] = 'critical'\n",
    "        elif 'major' in response_lower:\n",
    "            qc_info['severity'] = 'major'\n",
    "        elif 'minor' in response_lower:\n",
    "            qc_info['severity'] = 'minor'\n",
    "        elif qc_info['pass_fail'] == 'FAIL':\n",
    "            qc_info['severity'] = 'major'\n",
    "        elif found_defects:\n",
    "            qc_info['severity'] = 'minor'\n",
    "        else:\n",
    "            qc_info['severity'] = 'none'\n",
    "        \n",
    "        return qc_info\n",
    "    \n",
    "    def batch_analyze(\n",
    "        self,\n",
    "        image_paths: List[Union[str, Path]],\n",
    "        analysis_type: str = 'detailed_analysis',\n",
    "        show_progress: bool = True\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Analyze multiple cardboard images in batch.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        iterator = tqdm(image_paths, desc=\"Analyzing cardboard samples\") if show_progress else image_paths\n",
    "        \n",
    "        for image_path in iterator:\n",
    "            try:\n",
    "                result = self.analyze_cardboard(image_path, analysis_type)\n",
    "                results.append(result)\n",
    "                \n",
    "                if show_progress:\n",
    "                    qc = result['qc_analysis']\n",
    "                    status = qc.get('pass_fail', 'UNKNOWN')\n",
    "                    confidence = qc.get('confidence', 'N/A')\n",
    "                    iterator.set_postfix(status=status, confidence=f\"{confidence}%\" if confidence else \"N/A\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"❌ Error analyzing {image_path}: {e}\")\n",
    "                results.append({\n",
    "                    'image_path': str(image_path),\n",
    "                    'error': str(e),\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def generate_qc_report(self, save_path: Optional[str] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Generate comprehensive QC report from analysis history.\"\"\"\n",
    "        if not self.qc_history:\n",
    "            return {\"message\": \"No QC analysis data available\"}\n",
    "        \n",
    "        # Analyze results\n",
    "        total_samples = len(self.qc_history)\n",
    "        passed = sum(1 for h in self.qc_history if h.get('qc_analysis', {}).get('pass_fail') == 'PASS')\n",
    "        failed = sum(1 for h in self.qc_history if h.get('qc_analysis', {}).get('pass_fail') == 'FAIL')\n",
    "        \n",
    "        # Calculate averages\n",
    "        avg_inference_time = np.mean([h.get('inference_time', 0) for h in self.qc_history])\n",
    "        avg_confidence = np.mean([\n",
    "            h.get('qc_analysis', {}).get('confidence', 0) \n",
    "            for h in self.qc_history \n",
    "            if h.get('qc_analysis', {}).get('confidence') is not None\n",
    "        ]) if any(h.get('qc_analysis', {}).get('confidence') for h in self.qc_history) else 0\n",
    "        \n",
    "        # Common defects\n",
    "        all_defects = []\n",
    "        for h in self.qc_history:\n",
    "            defects = h.get('qc_analysis', {}).get('defects', [])\n",
    "            all_defects.extend(defects)\n",
    "        \n",
    "        defect_counts = {}\n",
    "        for defect in all_defects:\n",
    "            defect_counts[defect] = defect_counts.get(defect, 0) + 1\n",
    "        \n",
    "        report = {\n",
    "            'summary': {\n",
    "                'total_samples': total_samples,\n",
    "                'passed': passed,\n",
    "                'failed': failed,\n",
    "                'pass_rate': (passed / total_samples) * 100 if total_samples > 0 else 0,\n",
    "                'avg_inference_time': avg_inference_time,\n",
    "                'avg_confidence': avg_confidence\n",
    "            },\n",
    "            'defect_analysis': {\n",
    "                'common_defects': sorted(defect_counts.items(), key=lambda x: x[1], reverse=True),\n",
    "                'total_defects_found': len(all_defects)\n",
    "            },\n",
    "            'performance': self.engine.get_performance_stats(),\n",
    "            'generated_at': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Save report if path provided\n",
    "        if save_path:\n",
    "            with open(save_path, 'w') as f:\n",
    "                json.dump(report, f, indent=2)\n",
    "            logger.info(f\"📄 QC report saved to {save_path}\")\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def plot_qc_statistics(self, save_path: Optional[str] = None):\n",
    "        \"\"\"Plot QC analysis statistics.\"\"\"\n",
    "        if not self.qc_history:\n",
    "            print(\"No QC data to plot\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Pass/Fail distribution\n",
    "        pass_fail_data = [h.get('qc_analysis', {}).get('pass_fail') for h in self.qc_history]\n",
    "        pass_fail_counts = pd.Series(pass_fail_data).value_counts()\n",
    "        \n",
    "        axes[0, 0].pie(pass_fail_counts.values, labels=pass_fail_counts.index, autopct='%1.1f%%')\n",
    "        axes[0, 0].set_title('Pass/Fail Distribution')\n",
    "        \n",
    "        # Confidence scores\n",
    "        confidences = [\n",
    "            h.get('qc_analysis', {}).get('confidence', 0) \n",
    "            for h in self.qc_history \n",
    "            if h.get('qc_analysis', {}).get('confidence') is not None\n",
    "        ]\n",
    "        \n",
    "        if confidences:\n",
    "            axes[0, 1].hist(confidences, bins=20, alpha=0.7, color='skyblue')\n",
    "            axes[0, 1].set_title('Confidence Score Distribution')\n",
    "            axes[0, 1].set_xlabel('Confidence (%)')\n",
    "            axes[0, 1].set_ylabel('Frequency')\n",
    "        \n",
    "        # Inference times\n",
    "        inference_times = [h.get('inference_time', 0) for h in self.qc_history]\n",
    "        axes[1, 0].plot(inference_times, marker='o', alpha=0.7)\n",
    "        axes[1, 0].set_title('Inference Time Over Samples')\n",
    "        axes[1, 0].set_xlabel('Sample Number')\n",
    "        axes[1, 0].set_ylabel('Inference Time (s)')\n",
    "        \n",
    "        # Common defects\n",
    "        all_defects = []\n",
    "        for h in self.qc_history:\n",
    "            defects = h.get('qc_analysis', {}).get('defects', [])\n",
    "            all_defects.extend(defects)\n",
    "        \n",
    "        if all_defects:\n",
    "            defect_counts = pd.Series(all_defects).value_counts().head(10)\n",
    "            defect_counts.plot(kind='bar', ax=axes[1, 1], alpha=0.7)\n",
    "            axes[1, 1].set_title('Most Common Defects')\n",
    "            axes[1, 1].set_xlabel('Defect Type')\n",
    "            axes[1, 1].set_ylabel('Count')\n",
    "            axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"📊 QC statistics plot saved to {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "print(\"✅ Cardboard QC Processor ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Initialization and Configuration\n",
    "\n",
    "Set up your specific model paths and initialize the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure your specific model paths\n",
    "BASE_MODEL_PATH = r\"C:\\Users\\76135\\Desktop\\ADSYS-Cardboard-AI-Detection\\Yolo DS To Qwen DS\\Finetuned LoRA model\\V1 26-08-2025 Qwen 2.5vl_7b\\base_model\"\n",
    "LORA_MODEL_PATH = r\"C:\\Users\\76135\\Desktop\\ADSYS-Cardboard-AI-Detection\\Yolo DS To Qwen DS\\Finetuned LoRA model\\V1 26-08-2025 Qwen 2.5vl_7b\\lora_model\"\n",
    "TEST_IMAGES_PATH = r\"C:\\Users\\76135\\Desktop\\ADSYS-Cardboard-AI-Detection\\test_img\"\n",
    "\n",
    "# Verify paths exist\n",
    "print(\"🔍 Verifying model paths...\")\n",
    "base_path = Path(BASE_MODEL_PATH)\n",
    "lora_path = Path(LORA_MODEL_PATH)\n",
    "test_path = Path(TEST_IMAGES_PATH)\n",
    "\n",
    "if not base_path.exists():\n",
    "    print(f\"❌ Base model path not found: {base_path}\")\n",
    "    print(\"Please update BASE_MODEL_PATH to the correct location\")\n",
    "else:\n",
    "    print(f\"✅ Base model path verified: {base_path}\")\n",
    "\n",
    "if not lora_path.exists():\n",
    "    print(f\"❌ LoRA model path not found: {lora_path}\")\n",
    "    print(\"Please update LORA_MODEL_PATH to the correct location\")\n",
    "else:\n",
    "    print(f\"✅ LoRA model path verified: {lora_path}\")\n",
    "\n",
    "if not test_path.exists():\n",
    "    print(f\"❌ Test images path not found: {test_path}\")\n",
    "    print(\"Please update TEST_IMAGES_PATH to the correct location\")\n",
    "else:\n",
    "    test_images = list(test_path.glob(\"*.jpg\")) + list(test_path.glob(\"*.jpeg\")) + list(test_path.glob(\"*.png\")) + list(test_path.glob(\"*.JPG\"))\n",
    "    print(f\"✅ Test images path verified: {test_path} ({len(test_images)} images found)\")\n",
    "\n",
    "# Choose precision based on your RTX 3060\n",
    "PRECISION = ModelPrecisionConfig.recommend_config(6.0)  # 6GB VRAM\n",
    "print(f\"\\n🎯 Selected precision: {PRECISION.upper()}\")\n",
    "\n",
    "# Option to override precision (uncomment to use different precision)\n",
    "# PRECISION = 'fp16'  # Use this for maximum quality if memory allows\n",
    "# PRECISION = 'int4'  # Use this for maximum memory savings\n",
    "\n",
    "print(f\"\\n⚙️ Configuration Summary:\")\n",
    "print(f\"   Base Model: {BASE_MODEL_PATH}\")\n",
    "print(f\"   LoRA Adapter: {LORA_MODEL_PATH}\")\n",
    "print(f\"   Test Images: {TEST_IMAGES_PATH}\")\n",
    "print(f\"   Precision: {PRECISION}\")\n",
    "print(f\"   GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load Model with Full GPU Optimization\n",
    "\n",
    "Initialize and load the model with maximum GPU utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the inference engine\n",
    "print(\"🚀 Initializing Advanced LoRA Inference Engine...\")\n",
    "\n",
    "# Create the engine with full GPU optimization\n",
    "engine = AdvancedLoRAInferenceEngine(\n",
    "    base_model_path=BASE_MODEL_PATH,\n",
    "    lora_path=LORA_MODEL_PATH,\n",
    "    precision=PRECISION,\n",
    "    enable_compilation=True,  # Enable torch.compile for extra speed\n",
    "    memory_manager=memory_manager\n",
    ")\n",
    "\n",
    "print(\"\\n📥 Loading model - this may take several minutes...\")\n",
    "print(\"💡 This is equivalent to Ollama's num_gpu=999 (full GPU offloading)\")\n",
    "\n",
    "try:\n",
    "    # Load the model with memory tracking\n",
    "    start_time = time.time()\n",
    "    engine.load_model()\n",
    "    load_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n🎉 Model loaded successfully in {load_time:.1f} seconds!\")\n",
    "    \n",
    "    # Display final memory usage\n",
    "    memory_manager.log_memory_usage(\"model loaded\")\n",
    "    \n",
    "    # Initialize QC processor\n",
    "    qc_processor = CardboardQCProcessor(engine)\n",
    "    print(\"\\n✅ Cardboard QC Processor initialized!\")\n",
    "    print(\"\\n🎯 System ready for cardboard quality control inference!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading model: {e}\")\n",
    "    print(\"\\n🔧 Troubleshooting tips:\")\n",
    "    print(\"   1. Check if model paths are correct\")\n",
    "    print(\"   2. Ensure you have enough GPU/RAM memory\")\n",
    "    print(\"   3. Try a lower precision (int4 instead of int8)\")\n",
    "    print(\"   4. Check CUDA installation and compatibility\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Single Image Analysis\n",
    "\n",
    "Test the system with a single cardboard image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a test image\n",
    "test_images = list(Path(TEST_IMAGES_PATH).glob(\"*.jpg\")) + list(Path(TEST_IMAGES_PATH).glob(\"*.JPG\")) + list(Path(TEST_IMAGES_PATH).glob(\"*.png\"))\n",
    "\n",
    "if not test_images:\n",
    "    print(f\"❌ No test images found in {TEST_IMAGES_PATH}\")\n",
    "    print(\"Please add some cardboard images to test with\")\n",
    "else:\n",
    "    # Use the first available image\n",
    "    test_image = test_images[0]\n",
    "    print(f\"🖼️ Testing with image: {test_image.name}\")\n",
    "    \n",
    "    # Display the test image\n",
    "    img = Image.open(test_image)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Test Image: {test_image.name}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Perform detailed QC analysis\n",
    "    print(\"\\n🔍 Performing detailed quality control analysis...\")\n",
    "    \n",
    "    try:\n",
    "        result = qc_processor.analyze_cardboard(\n",
    "            test_image,\n",
    "            analysis_type='detailed_analysis'\n",
    "        )\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\n📊 Analysis Results:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        qc_analysis = result['qc_analysis']\n",
    "        print(f\"🎯 Decision: {qc_analysis.get('pass_fail', 'UNKNOWN')}\")\n",
    "        print(f\"📈 Confidence: {qc_analysis.get('confidence', 'N/A')}%\")\n",
    "        print(f\"⚠️ Defects Found: {', '.join(qc_analysis.get('defects', [])) or 'None'}\")\n",
    "        print(f\"🚨 Severity: {qc_analysis.get('severity', 'unknown').upper()}\")\n",
    "        \n",
    "        print(f\"\\n⏱️ Performance:\")\n",
    "        print(f\"   Inference Time: {result['inference_time']:.2f}s\")\n",
    "        print(f\"   Tokens/Second: {result['tokens_per_second']:.1f}\")\n",
    "        print(f\"   Input Tokens: {result['input_tokens']}\")\n",
    "        print(f\"   Output Tokens: {result['output_tokens']}\")\n",
    "        \n",
    "        print(f\"\\n📝 Detailed Analysis:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(result['response'])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Analysis failed: {e}\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Batch Processing Demo\n",
    "\n",
    "Process multiple cardboard images efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch process multiple images\n",
    "print(\"📦 Batch Processing Demo\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "if len(test_images) > 1:\n",
    "    # Select first few images for demo (limit to 5 to avoid long processing)\n",
    "    batch_images = test_images[:min(5, len(test_images))]\n",
    "    print(f\"Processing {len(batch_images)} images...\")\n",
    "    \n",
    "    # Optimize memory before batch processing\n",
    "    memory_manager.optimize_gpu_memory()\n",
    "    \n",
    "    # Process batch with pass/fail analysis (faster)\n",
    "    try:\n",
    "        batch_results = qc_processor.batch_analyze(\n",
    "            batch_images,\n",
    "            analysis_type='pass_fail_only',\n",
    "            show_progress=True\n",
    "        )\n",
    "        \n",
    "        # Summary of results\n",
    "        print(\"\\n📈 Batch Processing Summary:\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        passed = sum(1 for r in batch_results if r.get('qc_analysis', {}).get('pass_fail') == 'PASS')\n",
    "        failed = sum(1 for r in batch_results if r.get('qc_analysis', {}).get('pass_fail') == 'FAIL')\n",
    "        errors = sum(1 for r in batch_results if 'error' in r)\n",
    "        \n",
    "        total_time = sum(r.get('inference_time', 0) for r in batch_results if 'inference_time' in r)\n",
    "        avg_time = total_time / len(batch_results) if batch_results else 0\n",
    "        \n",
    "        print(f\"✅ Passed: {passed}\")\n",
    "        print(f\"❌ Failed: {failed}\")\n",
    "        print(f\"⚠️ Errors: {errors}\")\n",
    "        print(f\"⏱️ Total Time: {total_time:.1f}s\")\n",
    "        print(f\"📊 Average Time: {avg_time:.2f}s per image\")\n",
    "        \n",
    "        # Detailed results table\n",
    "        print(\"\\n📋 Detailed Results:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for i, result in enumerate(batch_results, 1):\n",
    "            if 'error' in result:\n",
    "                print(f\"{i:2d}. {Path(result['image_path']).name:25s} ERROR: {result['error']}\")\n",
    "            else:\n",
    "                qc = result.get('qc_analysis', {})\n",
    "                status = qc.get('pass_fail', 'UNKNOWN')\n",
    "                confidence = qc.get('confidence', 'N/A')\n",
    "                time_taken = result.get('inference_time', 0)\n",
    "                \n",
    "                status_icon = \"✅\" if status == 'PASS' else \"❌\" if status == 'FAIL' else \"❓\"\n",
    "                print(f\"{i:2d}. {Path(result['image_path']).name:25s} {status_icon} {status:6s} ({confidence}%) - {time_taken:.2f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Batch processing failed: {e}\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc())\n",
    "        \n",
    "else:\n",
    "    print(\"Only one test image available - add more images for batch processing demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Performance Analysis and Visualization\n",
    "\n",
    "Analyze system performance and create visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive QC report\n",
    "print(\"📊 Generating Performance Analysis...\")\n",
    "\n",
    "# Get QC report\n",
    "qc_report = qc_processor.generate_qc_report()\n",
    "\n",
    "if \"message\" not in qc_report:\n",
    "    # Display summary\n",
    "    summary = qc_report['summary']\n",
    "    print(\"\\n📈 Quality Control Summary:\")\n",
    "    print(\"=\" * 35)\n",
    "    print(f\"Total Samples Analyzed: {summary['total_samples']}\")\n",
    "    print(f\"Passed: {summary['passed']} ({summary['pass_rate']:.1f}%)\")\n",
    "    print(f\"Failed: {summary['failed']}\")\n",
    "    print(f\"Average Inference Time: {summary['avg_inference_time']:.2f}s\")\n",
    "    print(f\"Average Confidence: {summary['avg_confidence']:.1f}%\")\n",
    "    \n",
    "    # Performance statistics\n",
    "    perf = qc_report['performance']\n",
    "    print(\"\\n⚡ Performance Statistics:\")\n",
    "    print(\"=\" * 30)\n",
    "    print(f\"Total Inferences: {perf.get('total_inferences', 0)}\")\n",
    "    print(f\"Fastest Inference: {perf.get('min_time', 0):.2f}s\")\n",
    "    print(f\"Slowest Inference: {perf.get('max_time', 0):.2f}s\")\n",
    "    print(f\"Median Time: {perf.get('median_time', 0):.2f}s\")\n",
    "    print(f\"Time Std Dev: {perf.get('std_time', 0):.2f}s\")\n",
    "    \n",
    "    # Defect analysis\n",
    "    defects = qc_report['defect_analysis']\n",
    "    if defects['common_defects']:\n",
    "        print(\"\\n🔍 Common Defects Found:\")\n",
    "        print(\"=\" * 25)\n",
    "        for defect, count in defects['common_defects'][:5]:  # Top 5\n",
    "            print(f\"  {defect.title()}: {count} occurrences\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    print(\"\\n📊 Creating visualizations...\")\n",
    "    qc_processor.plot_qc_statistics()\n",
    "    \n",
    "    # Memory usage over time\n",
    "    print(\"\\n💾 Memory Usage Analysis:\")\n",
    "    memory_manager.plot_memory_history()\n",
    "    \n",
    "else:\n",
    "    print(qc_report['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Advanced Features Demo\n",
    "\n",
    "Demonstrate advanced features like custom prompts and comparative analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo advanced features\n",
    "print(\"🚀 Advanced Features Demo\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "if test_images:\n",
    "    test_image = test_images[0]\n",
    "    \n",
    "    print(\"\\n1️⃣ Custom Prompt Analysis\")\n",
    "    print(\"-\" * 25)\n",
    "    \n",
    "    custom_prompt = \"\"\"\n",
    "As a quality control expert, examine this cardboard sample and provide:\n",
    "1. A quality grade (A, B, C, D, or F)\n",
    "2. Three specific observations about the material condition\n",
    "3. Recommendation for use (Approved/Conditional/Rejected)\n",
    "4. Any safety concerns\n",
    "\n",
    "Format your response clearly with numbered points.\n",
    "    \"\"\".strip()\n",
    "    \n",
    "    try:\n",
    "        custom_result = qc_processor.analyze_cardboard(\n",
    "            test_image,\n",
    "            custom_prompt=custom_prompt\n",
    "        )\n",
    "        \n",
    "        print(\"Custom Analysis Result:\")\n",
    "        print(custom_result['response'])\n",
    "        print(f\"⏱️ Time: {custom_result['inference_time']:.2f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Custom analysis failed: {e}\")\n",
    "    \n",
    "    print(\"\\n2️⃣ Defect Detection Focus\")\n",
    "    print(\"-\" * 25)\n",
    "    \n",
    "    try:\n",
    "        defect_result = qc_processor.analyze_cardboard(\n",
    "            test_image,\n",
    "            analysis_type='defect_detection'\n",
    "        )\n",
    "        \n",
    "        print(\"Defect Detection Result:\")\n",
    "        print(defect_result['response'])\n",
    "        print(f\"⏱️ Time: {defect_result['inference_time']:.2f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Defect detection failed: {e}\")\n",
    "    \n",
    "    print(\"\\n3️⃣ Comparative Rating\")\n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "    try:\n",
    "        rating_result = qc_processor.analyze_cardboard(\n",
    "            test_image,\n",
    "            analysis_type='comparative_analysis'\n",
    "        )\n",
    "        \n",
    "        print(\"Comparative Rating Result:\")\n",
    "        print(rating_result['response'])\n",
    "        print(f\"⏱️ Time: {rating_result['inference_time']:.2f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Comparative analysis failed: {e}\")\n",
    "else:\n",
    "    print(\"No test images available for advanced demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Memory Optimization and Cleanup\n",
    "\n",
    "Demonstrate memory management and cleanup procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🧹 Memory Management Demo\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Show current memory usage\n",
    "print(\"\\n📊 Current Memory Usage:\")\n",
    "memory_manager.log_memory_usage(\"before cleanup\")\n",
    "\n",
    "# Demonstrate memory optimization\n",
    "print(\"\\n🔧 Running memory optimization...\")\n",
    "memory_manager.optimize_gpu_memory(aggressive=False)\n",
    "\n",
    "# Show memory after optimization\n",
    "print(\"\\n📊 Memory After Optimization:\")\n",
    "memory_manager.log_memory_usage(\"after optimization\")\n",
    "\n",
    "# Calculate optimal batch size\n",
    "print(\"\\n🎯 Batch Size Recommendation:\")\n",
    "optimal_batch = memory_manager.get_optimal_batch_size(\n",
    "    model_size_gb=3.0,  # Approximate model size\n",
    "    safety_factor=0.7   # Conservative safety factor\n",
    ")\n",
    "print(f\"Recommended batch size: {optimal_batch}\")\n",
    "\n",
    "# Show memory statistics over time\n",
    "stats = memory_manager.get_memory_stats()\n",
    "print(f\"\\n📈 Current GPU Utilization: {stats.gpu_utilization_percent:.1f}%\")\n",
    "print(f\"🆓 Free GPU Memory: {stats.gpu_free_gb:.2f}GB\")\n",
    "print(f\"🔄 RAM Usage: {stats.ram_used_percent:.1f}%\")\n",
    "\n",
    "# Save performance report\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "report_path = f\"cardboard_qc_report_{timestamp}.json\"\n",
    "final_report = qc_processor.generate_qc_report(save_path=report_path)\n",
    "\n",
    "print(f\"\\n💾 Performance report saved to: {report_path}\")\n",
    "\n",
    "# Cleanup option (uncomment to unload model)\n",
    "# print(\"\\n🔄 Unloading model to free memory...\")\n",
    "# engine.unload_model()\n",
    "# print(\"✅ Model unloaded - memory freed\")\n",
    "\n",
    "print(\"\\n✨ Memory management demo complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Production Usage Examples\n",
    "\n",
    "Examples of how to use this system in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🏭 Production Usage Examples\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "print(\"\"\"\n",
    "💡 How to Use This System in Production:\n",
    "\n",
    "1️⃣ **API Integration**:\n",
    "   - Wrap the qc_processor in a Flask/FastAPI endpoint\n",
    "   - Accept image uploads via HTTP POST\n",
    "   - Return JSON responses with QC results\n",
    "\n",
    "2️⃣ **Batch Processing Pipeline**:\n",
    "   - Monitor a directory for new cardboard images\n",
    "   - Process batches automatically using batch_analyze()\n",
    "   - Save results to database or CSV files\n",
    "\n",
    "3️⃣ **Real-time Quality Control**:\n",
    "   - Integrate with camera systems\n",
    "   - Process images as they're captured\n",
    "   - Trigger alerts for failed samples\n",
    "\n",
    "4️⃣ **Memory Management**:\n",
    "   - Use memory_manager.memory_tracking() for monitoring\n",
    "   - Run optimize_gpu_memory() periodically\n",
    "   - Implement automatic model reloading if memory issues occur\n",
    "\n",
    "5️⃣ **Performance Optimization**:\n",
    "   - Use torch.compile() for 10-30% speed improvement\n",
    "   - Batch similar images together for efficiency\n",
    "   - Cache frequently used prompts and configurations\n",
    "\"\"\")\n",
    "\n",
    "# Example production configuration\n",
    "production_config = {\n",
    "    'model_settings': {\n",
    "        'precision': PRECISION,\n",
    "        'enable_compilation': True,\n",
    "        'batch_size': memory_manager.get_optimal_batch_size(3.0, 0.7)\n",
    "    },\n",
    "    'qc_settings': {\n",
    "        'default_analysis_type': 'pass_fail_only',  # Fastest for production\n",
    "        'confidence_threshold': 85,  # Minimum confidence for auto-approval\n",
    "        'temperature': 0.3,  # Lower temperature for consistent results\n",
    "        'max_tokens': 256   # Sufficient for QC responses\n",
    "    },\n",
    "    'performance_monitoring': {\n",
    "        'log_memory_every_n_samples': 50,\n",
    "        'cleanup_memory_every_n_samples': 100,\n",
    "        'generate_report_every_n_samples': 500\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n⚙️ Recommended Production Configuration:\")\n",
    "print(json.dumps(production_config, indent=2))\n",
    "\n",
    "# Performance comparison with Ollama\n",
    "print(\"\\n📊 Performance Comparison with Ollama:\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "if hasattr(qc_processor, 'qc_history') and qc_processor.qc_history:\n",
    "    avg_time = np.mean([h.get('inference_time', 0) for h in qc_processor.qc_history])\n",
    "    avg_tokens_per_sec = np.mean([\n",
    "        h.get('tokens_per_second', 0) for h in qc_processor.qc_history \n",
    "        if h.get('tokens_per_second', 0) > 0\n",
    "    ])\n",
    "    \n",
    "    print(f\"🚀 Your System Performance:\")\n",
    "    print(f\"   Average inference time: {avg_time:.2f}s\")\n",
    "    print(f\"   Average tokens/second: {avg_tokens_per_sec:.1f}\")\n",
    "    print(f\"   Precision: {PRECISION.upper()}\")\n",
    "    print(f\"   GPU utilization: Full offloading (equivalent to num_gpu=999)\")\n",
    "    print(f\"\\n✅ Benefits over Ollama:\")\n",
    "    print(f\"   - Direct LoRA access (no GGUF conversion needed)\")\n",
    "    print(f\"   - Fine-grained memory control\")\n",
    "    print(f\"   - Custom precision options\")\n",
    "    print(f\"   - Specialized QC prompts and parsing\")\n",
    "    print(f\"   - Advanced monitoring and analytics\")\n",
    "else:\n",
    "    print(\"Run some inference examples above to see performance metrics\")\n",
    "\n",
    "print(\"\\n🎯 Next Steps for Production:\")\n",
    "print(\"1. Test with your specific cardboard images\")\n",
    "print(\"2. Tune prompts for your quality standards\")\n",
    "print(\"3. Set up automated monitoring and alerts\")\n",
    "print(\"4. Create API endpoints for integration\")\n",
    "print(\"5. Implement data logging and analytics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Troubleshooting and Advanced Configuration\n",
    "\n",
    "Common issues and their solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔧 Troubleshooting Guide\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "print(\"\"\"\n",
    "❌ **Out of Memory Errors**:\n",
    "   Solutions:\n",
    "   - Switch to lower precision: int4 instead of int8\n",
    "   - Reduce batch size in generation config\n",
    "   - Use CPU offloading: device_map='auto' with CPU fallback\n",
    "   - Run memory_manager.optimize_gpu_memory(aggressive=True)\n",
    "\n",
    "⏱️ **Slow Inference Speed**:\n",
    "   Solutions:\n",
    "   - Ensure torch.compile() is enabled\n",
    "   - Use fp16 precision if memory allows\n",
    "   - Reduce max_new_tokens in generation config\n",
    "   - Enable flash_attention_2 if available\n",
    "   - Check GPU utilization with nvidia-smi\n",
    "\n",
    "🔄 **Model Loading Issues**:\n",
    "   Solutions:\n",
    "   - Verify all model files are present\n",
    "   - Check CUDA version compatibility\n",
    "   - Try loading without quantization first\n",
    "   - Ensure transformers and peft versions are compatible\n",
    "\n",
    "📝 **Poor Quality Results**:\n",
    "   Solutions:\n",
    "   - Adjust temperature (lower for more consistent results)\n",
    "   - Modify prompts to be more specific\n",
    "   - Check if LoRA adapter is properly loaded\n",
    "   - Verify training data quality and relevance\n",
    "\n",
    "🔗 **Integration Issues**:\n",
    "   Solutions:\n",
    "   - Wrap in try-catch blocks for production\n",
    "   - Implement health checks and auto-restart\n",
    "   - Use memory monitoring to prevent crashes\n",
    "   - Set up logging for debugging\n",
    "\"\"\")\n",
    "\n",
    "# System diagnostics\n",
    "print(\"\\n🔍 System Diagnostics\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "# Check CUDA status\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU: {torch.cuda.current_device()}\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# Check memory status\n",
    "current_stats = memory_manager.get_memory_stats()\n",
    "print(f\"\\nGPU Memory Allocated: {current_stats.gpu_allocated_gb:.2f} GB\")\n",
    "print(f\"GPU Memory Free: {current_stats.gpu_free_gb:.2f} GB\")\n",
    "print(f\"RAM Usage: {current_stats.ram_used_percent:.1f}%\")\n",
    "\n",
    "# Check package versions\n",
    "print(f\"\\nPackage Versions:\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Transformers: {transformers.__version__}\")\n",
    "print(f\"PEFT: {peft.__version__}\")\n",
    "print(f\"Accelerate: {accelerate.__version__}\")\n",
    "\n",
    "# Check model status\n",
    "if hasattr(engine, 'is_loaded'):\n",
    "    print(f\"\\nModel Status: {'Loaded' if engine.is_loaded else 'Not Loaded'}\")\n",
    "    if engine.is_loaded:\n",
    "        print(f\"Precision: {engine.precision}\")\n",
    "        print(f\"Compilation: {'Enabled' if engine.compiled_model else 'Disabled'}\")\n",
    "        print(f\"Device: {engine.device}\")\n",
    "\n",
    "print(\"\\n✅ Diagnostics complete\")\n",
    "\n",
    "# Performance tuning suggestions\n",
    "print(\"\\n🚀 Performance Tuning Suggestions:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3 if torch.cuda.is_available() else 0\n",
    "\n",
    "if gpu_memory_gb >= 12:\n",
    "    print(\"🎯 Your GPU has plenty of memory - consider:\")\n",
    "    print(\"   - Using fp16 precision for best quality\")\n",
    "    print(\"   - Increasing batch size for efficiency\")\n",
    "    print(\"   - Running multiple models simultaneously\")\n",
    "elif gpu_memory_gb >= 8:\n",
    "    print(\"⚖️ Your GPU has good memory - current settings are optimal\")\n",
    "    print(\"   - int8 or bf16 precision recommended\")\n",
    "    print(\"   - Moderate batch sizes work well\")\n",
    "elif gpu_memory_gb >= 6:\n",
    "    print(\"💡 Your RTX 3060 setup - recommendations:\")\n",
    "    print(\"   - int8 precision (current) is optimal\")\n",
    "    print(\"   - Keep batch size small (1-2)\")\n",
    "    print(\"   - Enable aggressive memory cleanup\")\n",
    "    print(\"   - Consider int4 for maximum memory savings\")\n",
    "else:\n",
    "    print(\"⚠️ Limited GPU memory - consider:\")\n",
    "    print(\"   - int4 precision for maximum savings\")\n",
    "    print(\"   - CPU offloading for large models\")\n",
    "    print(\"   - Frequent memory cleanup\")\n",
    "\n",
    "print(\"\\n📚 For more help, check the documentation or create an issue on GitHub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Save Configuration and Cleanup\n",
    "\n",
    "Save your configuration and optionally clean up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"💾 Saving Configuration and Cleanup\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Save current configuration\n",
    "config_to_save = {\n",
    "    'model_paths': {\n",
    "        'base_model': BASE_MODEL_PATH,\n",
    "        'lora_model': LORA_MODEL_PATH,\n",
    "        'test_images': TEST_IMAGES_PATH\n",
    "    },\n",
    "    'model_config': {\n",
    "        'precision': PRECISION,\n",
    "        'enable_compilation': True,\n",
    "        'device': str(engine.device) if hasattr(engine, 'device') else 'auto'\n",
    "    },\n",
    "    'system_info': {\n",
    "        'gpu_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU',\n",
    "        'gpu_memory_gb': torch.cuda.get_device_properties(0).total_memory / 1024**3 if torch.cuda.is_available() else 0,\n",
    "        'pytorch_version': torch.__version__,\n",
    "        'transformers_version': transformers.__version__,\n",
    "        'peft_version': peft.__version__\n",
    "    },\n",
    "    'performance_summary': engine.get_performance_stats() if hasattr(engine, 'get_performance_stats') else {},\n",
    "    'generated_at': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "# Save configuration\n",
    "config_filename = f\"cardboard_qc_config_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(config_filename, 'w') as f:\n",
    "    json.dump(config_to_save, f, indent=2)\n",
    "\n",
    "print(f\"✅ Configuration saved to: {config_filename}\")\n",
    "\n",
    "# Final memory report\n",
    "print(\"\\n📊 Final Memory Report:\")\n",
    "memory_manager.log_memory_usage(\"session end\")\n",
    "\n",
    "# Option to unload model (uncomment to free memory)\n",
    "print(\"\\n🔄 Model Cleanup Options:\")\n",
    "print(\"To free GPU memory, run: engine.unload_model()\")\n",
    "print(\"To optimize memory: memory_manager.optimize_gpu_memory(aggressive=True)\")\n",
    "\n",
    "# Cleanup option\n",
    "cleanup_choice = input(\"\\n🗑️ Unload model to free memory? (y/n): \").lower().strip()\n",
    "if cleanup_choice in ['y', 'yes']:\n",
    "    print(\"🧹 Unloading model...\")\n",
    "    engine.unload_model()\n",
    "    print(\"✅ Model unloaded successfully\")\n",
    "    memory_manager.log_memory_usage(\"after cleanup\")\n",
    "else:\n",
    "    print(\"ℹ️ Model remains loaded for continued use\")\n",
    "\n",
    "print(\"\\n🎉 Cardboard QC GPU Inference Setup Complete!\")\n",
    "print(\"\\n📋 Summary of what you achieved:\")\n",
    "print(\"   ✅ Full GPU offloading (equivalent to Ollama num_gpu=999)\")\n",
    "print(\"   ✅ Direct LoRA adapter usage without merging\")\n",
    "print(\"   ✅ Advanced memory management and monitoring\")\n",
    "print(\"   ✅ Specialized cardboard quality control prompts\")\n",
    "print(\"   ✅ Batch processing capabilities\")\n",
    "print(\"   ✅ Performance tracking and optimization\")\n",
    "print(\"   ✅ Production-ready error handling\")\n",
    "print(\"\\n🚀 Your system is ready for production cardboard quality control!\")\n",
    "\n",
    "# Final tips\n",
    "print(\"\\n💡 Next Steps:\")\n",
    "print(\"1. Test with your production cardboard images\")\n",
    "print(\"2. Fine-tune prompts for your specific quality standards\")\n",
    "print(\"3. Set up automated processing pipelines\")\n",
    "print(\"4. Monitor performance and adjust settings as needed\")\n",
    "print(f\"5. Refer to saved configuration: {config_filename}\")\n",
    "\n",
    "print(\"\\n📖 For questions or improvements, check the troubleshooting section above.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
