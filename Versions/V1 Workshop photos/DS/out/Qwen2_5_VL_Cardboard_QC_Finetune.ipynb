{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/your-username/your-repo/blob/main/Qwen2_5_VL_Cardboard_QC_Finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# Qwen2.5-VL Fine-tuning for Cardboard Quality Control 📦🔍\n",
    "\n",
    "<div class=\"align-center\">\n",
    "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "</div>\n",
    "\n",
    "Fine-tune Qwen2.5-VL (7B) for automated cardboard bundle quality control using your custom dataset.\n",
    "\n",
    "**Dataset**: `Cong2612/cardboard-qc-dataset` (168 samples)\n",
    "**Task**: Binary classification - Pass (flat) vs Fail (warped)\n",
    "**Model**: Vision-Language model for industrial quality control\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "installation"
   },
   "source": [
    "## 🔧 Installation & Setup\n",
    "\n",
    "Install Unsloth and required dependencies. This will take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, re\n",
    "import torch\n",
    "\n",
    "# Check if we're in Colab\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Colab-specific installation\n",
    "    v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
    "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
    "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "    \n",
    "# Additional packages for evaluation\n",
    "!pip install scikit-learn\n",
    "\n",
    "print(\"✅ Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_setup"
   },
   "source": [
    "## 🚀 Model Setup\n",
    "\n",
    "Load Qwen2.5-VL-7B with 4-bit quantization and add LoRA adapters for efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_model"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastVisionModel\n",
    "import torch\n",
    "\n",
    "print(\"🔧 Setting up Qwen2.5-VL for Cardboard Quality Control Fine-tuning\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load the base model with 4-bit quantization\n",
    "print(\"📥 Loading Qwen2.5-VL-7B model...\")\n",
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    \"unsloth/Qwen2.5-VL-7B-Instruct-bnb-4bit\",\n",
    "    load_in_4bit=True,  # 4-bit quantization for memory efficiency\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Memory optimization\n",
    ")\n",
    "\n",
    "print(\"✅ Base model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "add_lora"
   },
   "outputs": [],
   "source": [
    "# Add LoRA adapters for parameter-efficient fine-tuning\n",
    "print(\"🔧 Adding LoRA adapters...\")\n",
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers=True,      # Fine-tune vision layers for cardboard images\n",
    "    finetune_language_layers=True,    # Fine-tune language for QC assessment\n",
    "    finetune_attention_modules=True,  # Fine-tune attention layers\n",
    "    finetune_mlp_modules=True,        # Fine-tune MLP layers\n",
    "\n",
    "    r=16,             # LoRA rank - balance between accuracy and efficiency\n",
    "    lora_alpha=16,    # LoRA alpha - recommended to match r\n",
    "    lora_dropout=0,   # No dropout for stable training\n",
    "    bias=\"none\",\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "print(\"✅ LoRA adapters added successfully!\")\n",
    "print(\"📊 Model ready for fine-tuning on cardboard QC task\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_loading"
   },
   "source": [
    "## 📊 Dataset Loading & Preparation\n",
    "\n",
    "Load the cardboard QC dataset and convert it to the format required for vision-language training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_dataset"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(\"📊 Loading Cardboard Quality Control Dataset...\")\n",
    "\n",
    "# Load your uploaded dataset from Hugging Face Hub\n",
    "dataset = load_dataset(\"Cong2612/cardboard-qc-dataset\")\n",
    "\n",
    "print(f\"✅ Dataset loaded successfully!\")\n",
    "print(f\"   Train samples: {len(dataset['train'])}\")\n",
    "print(f\"   Validation samples: {len(dataset['validation'])}\")\n",
    "print(f\"   Test samples: {len(dataset['test'])}\")\n",
    "\n",
    "# Display example data\n",
    "example = dataset['train'][0]\n",
    "print(f\"\\n📝 Example data point:\")\n",
    "print(f\"   Filename: {example['filename']}\")\n",
    "print(f\"   Label: {example['label']}\")\n",
    "print(f\"   Reason: {example['reason']}\")\n",
    "print(f\"   Image size: {example['image'].size}\")\n",
    "print(f\"   Conversations: {len(example['conversations'])} messages\")\n",
    "\n",
    "# Show the conversation structure\n",
    "print(f\"\\n💬 Conversation example:\")\n",
    "for i, msg in enumerate(example['conversations']):\n",
    "    print(f\"   {i+1}. {msg['role']}: {msg['content'][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "display_image"
   },
   "outputs": [],
   "source": [
    "# Display sample images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "fig.suptitle('Sample Cardboard Bundle Images', fontsize=16)\n",
    "\n",
    "for i in range(3):\n",
    "    sample = dataset['train'][i]\n",
    "    axes[i].imshow(sample['image'])\n",
    "    axes[i].set_title(f\"{sample['label']}\\n{sample['filename'][:20]}...\", fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show label distribution\n",
    "train_labels = [sample['label'] for sample in dataset['train']]\n",
    "pass_count = train_labels.count('Pass')\n",
    "fail_count = train_labels.count('Fail')\n",
    "\n",
    "print(f\"\\n📈 Training Set Label Distribution:\")\n",
    "print(f\"   Pass (flat bundles): {pass_count} ({pass_count/len(train_labels)*100:.1f}%)\")\n",
    "print(f\"   Fail (warped bundles): {fail_count} ({fail_count/len(train_labels)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_conversion"
   },
   "source": [
    "## 🔄 Data Format Conversion\n",
    "\n",
    "Convert the dataset from your conversation format to the Unsloth vision training format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "convert_dataset"
   },
   "outputs": [],
   "source": [
    "def convert_cardboard_to_conversation(sample):\n",
    "    \"\"\"\n",
    "    Convert cardboard QC data to Unsloth conversation format.\n",
    "    Your dataset conversations are already well-structured, just need reformatting.\n",
    "    \"\"\"\n",
    "    # Extract conversations from your dataset format\n",
    "    conversations = sample[\"conversations\"]\n",
    "    \n",
    "    # Convert to Unsloth format\n",
    "    messages = []\n",
    "    for msg in conversations:\n",
    "        if msg[\"role\"] == \"user\":\n",
    "            # User message with text + image\n",
    "            messages.append({\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": msg[\"content\"]},\n",
    "                    {\"type\": \"image\", \"image\": sample[\"image\"]}\n",
    "                ]\n",
    "            })\n",
    "        else:  # assistant\n",
    "            # Assistant response with text only\n",
    "            messages.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": msg[\"content\"]}\n",
    "                ]\n",
    "            })\n",
    "    \n",
    "    return {\"messages\": messages}\n",
    "\n",
    "# Convert the training dataset\n",
    "print(\"🔄 Converting dataset to training format...\")\n",
    "train_dataset = dataset['train']\n",
    "converted_train = [convert_cardboard_to_conversation(sample) for sample in train_dataset]\n",
    "\n",
    "# Also convert validation set for potential evaluation\n",
    "val_dataset = dataset['validation']\n",
    "converted_val = [convert_cardboard_to_conversation(sample) for sample in val_dataset]\n",
    "\n",
    "print(f\"✅ Dataset conversion complete!\")\n",
    "print(f\"   Converted train samples: {len(converted_train)}\")\n",
    "print(f\"   Converted validation samples: {len(converted_val)}\")\n",
    "\n",
    "# Display converted example\n",
    "print(f\"\\n📋 Converted conversation structure:\")\n",
    "example_conv = converted_train[0]['messages']\n",
    "for i, msg in enumerate(example_conv):\n",
    "    print(f\"   {i+1}. Role: {msg['role']}\")\n",
    "    print(f\"      Content items: {len(msg['content'])}\")\n",
    "    for j, content in enumerate(msg['content']):\n",
    "        if content['type'] == 'text':\n",
    "            print(f\"         Text: {content['text'][:60]}...\")\n",
    "        else:\n",
    "            print(f\"         Image: {content['type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pre_training_test"
   },
   "source": [
    "## 🧪 Pre-Training Model Test\n",
    "\n",
    "Let's test the model before fine-tuning to see its baseline performance on cardboard QC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_before_training"
   },
   "outputs": [],
   "source": [
    "print(\"🧪 Testing model before fine-tuning...\")\n",
    "FastVisionModel.for_inference(model)  # Switch to inference mode\n",
    "\n",
    "# Test with a sample from the dataset\n",
    "test_sample = train_dataset[5]  # Pick sample 5\n",
    "test_image = test_sample[\"image\"]\n",
    "test_instruction = \"Analyze this cardboard bundle image. Is the bundle flat or warped? Provide your assessment.\"\n",
    "\n",
    "print(f\"🔍 Testing on: {test_sample['filename']}\")\n",
    "print(f\"📝 True label: {test_sample['label']} - {test_sample['reason']}\")\n",
    "\n",
    "# Prepare input\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": test_instruction}\n",
    "    ]}\n",
    "]\n",
    "\n",
    "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = tokenizer(\n",
    "    test_image,\n",
    "    input_text,\n",
    "    add_special_tokens=False,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "print(\"\\n🤖 Model response BEFORE training:\")\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "_ = model.generate(\n",
    "    **inputs, \n",
    "    streamer=text_streamer, \n",
    "    max_new_tokens=128,\n",
    "    use_cache=True, \n",
    "    temperature=1.0, \n",
    "    min_p=0.1\n",
    ")\n",
    "\n",
    "# Display the test image\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(test_image)\n",
    "plt.title(f\"Test Image: {test_sample['label']} - {test_sample['filename'][:30]}...\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training_setup"
   },
   "source": [
    "## 🏋️ Training Setup\n",
    "\n",
    "Configure the trainer with optimized hyperparameters for cardboard quality control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_trainer"
   },
   "outputs": [],
   "source": [
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "print(\"🏋️ Setting up training configuration...\")\n",
    "\n",
    "# Switch model to training mode\n",
    "FastVisionModel.for_training(model)\n",
    "\n",
    "# Training configuration optimized for cardboard QC\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=UnslothVisionDataCollator(model, tokenizer),  # Essential for vision training\n",
    "    train_dataset=converted_train,\n",
    "    args=SFTConfig(\n",
    "        per_device_train_batch_size=2,        # Adjust based on GPU memory\n",
    "        gradient_accumulation_steps=4,         # Effective batch size = 2 * 4 = 8\n",
    "        warmup_steps=10,                       # Warm-up steps for stable training\n",
    "        num_train_epochs=3,                    # Full training epochs\n",
    "        # max_steps=60,                        # Use this for quick testing instead\n",
    "        learning_rate=2e-4,                    # Learning rate for LoRA\n",
    "        logging_steps=5,                       # Log every 5 steps\n",
    "        optim=\"adamw_8bit\",                    # 8-bit optimizer for memory efficiency\n",
    "        weight_decay=0.01,                     # Regularization\n",
    "        lr_scheduler_type=\"linear\",            # Learning rate schedule\n",
    "        seed=3407,                             # For reproducibility\n",
    "        output_dir=\"outputs\",                  # Checkpoint directory\n",
    "        report_to=\"none\",                      # Change to \"wandb\" if using W&B\n",
    "        save_steps=50,                         # Save checkpoint every 50 steps\n",
    "        save_total_limit=2,                    # Keep only 2 checkpoints\n",
    "        \n",
    "        # Essential settings for vision fine-tuning\n",
    "        remove_unused_columns=False,\n",
    "        dataset_text_field=\"\",\n",
    "        dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "        max_length=2048,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"✅ Trainer configured successfully!\")\n",
    "\n",
    "# Show memory statistics before training\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "\n",
    "print(f\"\\n💾 Memory Statistics:\")\n",
    "print(f\"   GPU: {gpu_stats.name}\")\n",
    "print(f\"   Max memory: {max_memory} GB\")\n",
    "print(f\"   Reserved memory: {start_gpu_memory} GB\")\n",
    "print(f\"   Available memory: {max_memory - start_gpu_memory:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## 🚀 Model Training\n",
    "\n",
    "Start fine-tuning! This will take approximately 15-30 minutes depending on your GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_model"
   },
   "outputs": [],
   "source": [
    "print(\"🚀 Starting fine-tuning...\")\n",
    "print(\"⏰ This will take some time. Grab a coffee! ☕\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Start training\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"\\n🎉 Training Complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training_stats"
   },
   "outputs": [],
   "source": [
    "# Show training statistics\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "\n",
    "print(f\"📈 Training Statistics:\")\n",
    "print(f\"   Training time: {trainer_stats.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"   Training time: {trainer_stats.metrics['train_runtime']/60:.2f} minutes\")\n",
    "print(f\"   Peak memory: {used_memory} GB\")\n",
    "print(f\"   Peak memory for training: {used_memory_for_lora} GB\")\n",
    "print(f\"   Peak memory %: {used_percentage}%\")\n",
    "print(f\"   Training memory %: {lora_percentage}%\")\n",
    "\n",
    "# Show training loss if available\n",
    "if 'train_loss' in trainer_stats.metrics:\n",
    "    print(f\"   Final training loss: {trainer_stats.metrics['train_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "post_training_test"
   },
   "source": [
    "## 🧪 Post-Training Model Test\n",
    "\n",
    "Let's test the fine-tuned model to see the improvement!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_after_training"
   },
   "outputs": [],
   "source": [
    "print(\"🧪 Testing fine-tuned model...\")\n",
    "FastVisionModel.for_inference(model)  # Switch to inference mode\n",
    "\n",
    "# Test with the same sample as before\n",
    "inputs = tokenizer(\n",
    "    test_image,\n",
    "    input_text,\n",
    "    add_special_tokens=False,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "print(f\"🔍 Testing on same image: {test_sample['filename']}\")\n",
    "print(f\"📝 True label: {test_sample['label']} - {test_sample['reason']}\")\n",
    "print(\"\\n🤖 Model response AFTER fine-tuning:\")\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "_ = model.generate(\n",
    "    **inputs, \n",
    "    streamer=text_streamer, \n",
    "    max_new_tokens=128,\n",
    "    use_cache=True, \n",
    "    temperature=0.7,  # Slightly lower for more focused responses\n",
    "    min_p=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_multiple_samples"
   },
   "outputs": [],
   "source": [
    "# Test on a few more samples to see consistency\n",
    "print(\"\\n🎯 Testing on multiple samples...\")\n",
    "test_indices = [0, 10, 20]  # Test different samples\n",
    "\n",
    "for i, idx in enumerate(test_indices):\n",
    "    sample = train_dataset[idx]\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": test_instruction}\n",
    "        ]}\n",
    "    ]\n",
    "    \n",
    "    input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "    inputs = tokenizer(\n",
    "        sample[\"image\"],\n",
    "        input_text,\n",
    "        add_special_tokens=False,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    print(f\"\\n📝 Sample {i+1}: {sample['filename'][:30]}...\")\n",
    "    print(f\"   True: {sample['label']} - {sample['reason']}\")\n",
    "    print(f\"   Response: \", end=\"\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=80,\n",
    "            use_cache=True, \n",
    "            temperature=0.7, \n",
    "            min_p=0.1,\n",
    "            do_sample=True\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    prompt_len = len(tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True))\n",
    "    generated_text = response[prompt_len:].strip()\n",
    "    print(generated_text[:100] + \"...\" if len(generated_text) > 100 else generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation"
   },
   "source": [
    "## 📊 Model Evaluation\n",
    "\n",
    "Evaluate the fine-tuned model on the validation set to measure performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate_model"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def extract_prediction(response_text):\n",
    "    \"\"\"Extract Pass/Fail prediction from model response.\"\"\"\n",
    "    response_lower = response_text.lower()\n",
    "    \n",
    "    # Look for clear indicators\n",
    "    if 'pass' in response_lower and 'fail' not in response_lower:\n",
    "        return \"Pass\"\n",
    "    elif 'fail' in response_lower and 'pass' not in response_lower:\n",
    "        return \"Fail\"\n",
    "    elif 'flat' in response_lower and 'warp' not in response_lower:\n",
    "        return \"Pass\"\n",
    "    elif 'warp' in response_lower and 'flat' not in response_lower:\n",
    "        return \"Fail\"\n",
    "    else:\n",
    "        # Check for more subtle indicators\n",
    "        pass_count = sum(1 for word in ['good', 'acceptable', 'appears flat', 'looks flat'] \n",
    "                        if word in response_lower)\n",
    "        fail_count = sum(1 for word in ['bad', 'unacceptable', 'appears warped', 'looks warped'] \n",
    "                        if word in response_lower)\n",
    "        \n",
    "        if pass_count > fail_count:\n",
    "            return \"Pass\"\n",
    "        elif fail_count > pass_count:\n",
    "            return \"Fail\"\n",
    "        else:\n",
    "            return \"Unknown\"\n",
    "\n",
    "def evaluate_on_validation():\n",
    "    \"\"\"Evaluate model on validation set.\"\"\"\n",
    "    print(\"📊 Evaluating on validation set...\")\n",
    "    \n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    val_samples = min(10, len(val_dataset))  # Limit to 10 for speed in Colab\n",
    "    \n",
    "    for i in range(val_samples):\n",
    "        sample = val_dataset[i]\n",
    "        \n",
    "        # Prepare input\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": test_instruction}\n",
    "            ]}\n",
    "        ]\n",
    "        \n",
    "        input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "        inputs = tokenizer(\n",
    "            sample[\"image\"],\n",
    "            input_text,\n",
    "            add_special_tokens=False,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(\"cuda\")\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=100,\n",
    "                use_cache=True,\n",
    "                temperature=0.3,  # Lower temperature for consistent evaluation\n",
    "                min_p=0.05,\n",
    "                do_sample=True\n",
    "            )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        prompt_len = len(tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True))\n",
    "        generated_text = response[prompt_len:].strip()\n",
    "        \n",
    "        # Extract prediction\n",
    "        predicted_label = extract_prediction(generated_text)\n",
    "        true_label = sample['label']\n",
    "        \n",
    "        predictions.append(predicted_label)\n",
    "        true_labels.append(true_label)\n",
    "        \n",
    "        print(f\"   {i+1:2d}. {sample['filename'][:25]:25} | True: {true_label:4} | Pred: {predicted_label:4} | {'✓' if predicted_label == true_label else '✗'}\")\n",
    "    \n",
    "    return predictions, true_labels\n",
    "\n",
    "# Run evaluation\n",
    "predictions, true_labels = evaluate_on_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "show_metrics"
   },
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "valid_indices = [i for i, pred in enumerate(predictions) if pred in [\"Pass\", \"Fail\"]]\n",
    "valid_predictions = [predictions[i] for i in valid_indices]\n",
    "valid_true_labels = [true_labels[i] for i in valid_indices]\n",
    "\n",
    "if valid_predictions:\n",
    "    accuracy = accuracy_score(valid_true_labels, valid_predictions)\n",
    "    \n",
    "    print(f\"\\n📈 Evaluation Results:\")\n",
    "    print(f\"   Samples evaluated: {len(valid_predictions)}/{len(predictions)}\")\n",
    "    print(f\"   Accuracy: {accuracy:.3f} ({accuracy*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n📊 Classification Report:\")\n",
    "    print(classification_report(valid_true_labels, valid_predictions))\n",
    "    \n",
    "    # Count correct predictions\n",
    "    correct = sum(1 for p, t in zip(valid_predictions, valid_true_labels) if p == t)\n",
    "    print(f\"\\n✅ Correct predictions: {correct}/{len(valid_predictions)}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No valid predictions found. Model may need more training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_saving"
   },
   "source": [
    "## 💾 Save the Fine-tuned Model\n",
    "\n",
    "Save the LoRA adapters for later use or deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_model"
   },
   "outputs": [],
   "source": [
    "print(\"💾 Saving fine-tuned model...\")\n",
    "\n",
    "# Save LoRA adapters locally\n",
    "model.save_pretrained(\"cardboard_qc_lora\")\n",
    "tokenizer.save_pretrained(\"cardboard_qc_lora\")\n",
    "\n",
    "print(\"✅ LoRA adapters saved to 'cardboard_qc_lora' directory\")\n",
    "\n",
    "# Check what was saved\n",
    "import os\n",
    "saved_files = os.listdir(\"cardboard_qc_lora\")\n",
    "print(f\"📁 Saved files: {saved_files}\")\n",
    "\n",
    "# Optional: Zip the model for download\n",
    "!zip -r cardboard_qc_lora.zip cardboard_qc_lora/\n",
    "print(\"📦 Model zipped as 'cardboard_qc_lora.zip' for download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "optional_hub_upload"
   },
   "source": [
    "## 📤 Optional: Upload to Hugging Face Hub\n",
    "\n",
    "Upload your fine-tuned model to Hugging Face Hub for easy sharing and deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_to_hub"
   },
   "outputs": [],
   "source": [
    "# Set to True if you want to upload to HuggingFace Hub\n",
    "UPLOAD_TO_HUB = False  # Change to True if you want to upload\n",
    "\n",
    "if UPLOAD_TO_HUB:\n",
    "    # Configure these settings\n",
    "    HF_TOKEN = \"your_huggingface_token_here\"  # Get from https://huggingface.co/settings/tokens\n",
    "    MODEL_NAME = \"your-username/qwen2.5-vl-cardboard-qc-lora\"  # Change to your desired name\n",
    "    \n",
    "    print(f\"📤 Uploading model to Hugging Face Hub: {MODEL_NAME}\")\n",
    "    \n",
    "    try:\n",
    "        model.push_to_hub(\n",
    "            MODEL_NAME,\n",
    "            token=HF_TOKEN,\n",
    "            private=False  # Set to True for private repository\n",
    "        )\n",
    "        tokenizer.push_to_hub(\n",
    "            MODEL_NAME,\n",
    "            token=HF_TOKEN,\n",
    "            private=False\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Model uploaded successfully!\")\n",
    "        print(f\"🔗 Model URL: https://huggingface.co/{MODEL_NAME}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Upload failed: {e}\")\n",
    "        print(\"Make sure to set your HF_TOKEN and MODEL_NAME correctly\")\n",
    "        \n",
    "else:\n",
    "    print(\"⏭️  Skipping upload to Hugging Face Hub\")\n",
    "    print(\"💡 Set UPLOAD_TO_HUB = True above if you want to upload\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load_and_test"
   },
   "source": [
    "## 🔄 Load and Test Saved Model\n",
    "\n",
    "Verify that the saved model can be loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_loading"
   },
   "outputs": [],
   "source": [
    "# Test loading the saved model (optional - uncomment if you want to test)\n",
    "TEST_LOADING = False  # Set to True if you want to test loading\n",
    "\n",
    "if TEST_LOADING:\n",
    "    print(\"🔄 Testing model loading...\")\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    del model, tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Load the saved model\n",
    "    from unsloth import FastVisionModel\n",
    "    model, tokenizer = FastVisionModel.from_pretrained(\n",
    "        model_name=\"cardboard_qc_lora\",\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "    FastVisionModel.for_inference(model)\n",
    "    \n",
    "    # Quick test\n",
    "    test_sample = train_dataset[0]\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": \"Analyze this cardboard bundle. Is it flat or warped?\"}\n",
    "        ]}\n",
    "    ]\n",
    "    \n",
    "    input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "    inputs = tokenizer(\n",
    "        test_sample[\"image\"],\n",
    "        input_text,\n",
    "        add_special_tokens=False,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    print(\"🧪 Testing loaded model:\")\n",
    "    text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "    _ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=50)\n",
    "    \n",
    "    print(\"\\n✅ Model loading test successful!\")\n",
    "    \n",
    "else:\n",
    "    print(\"⏭️  Skipping model loading test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## 🎉 Fine-tuning Complete!\n",
    "\n",
    "### Summary\n",
    "\n",
    "✅ **Model**: Qwen2.5-VL-7B fine-tuned for cardboard quality control\n",
    "✅ **Dataset**: 117 training samples, 16 validation samples\n",
    "✅ **Task**: Binary classification (Pass/Fail for cardboard bundles)\n",
    "✅ **Training**: 3 epochs with LoRA adapters\n",
    "✅ **Output**: Saved LoRA adapters ready for deployment\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **📊 Evaluation**: Test on more validation samples for thorough evaluation\n",
    "2. **🚀 Deployment**: Use the saved model for production quality control\n",
    "3. **📈 Improvement**: Collect more data and retrain if needed\n",
    "4. **🔧 Integration**: Build API endpoints for real-time quality assessment\n",
    "\n",
    "### Usage Example\n",
    "\n",
    "```python\n",
    "# To load your fine-tuned model later:\n",
    "from unsloth import FastVisionModel\n",
    "\n",
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    model_name=\"cardboard_qc_lora\",  # or your HuggingFace repo\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "FastVisionModel.for_inference(model)\n",
    "\n",
    "# Then use for prediction on new cardboard images!\n",
    "```\n",
    "\n",
    "### 🎯 Your Model's Specialty\n",
    "\n",
    "Your fine-tuned Qwen2.5-VL model now specializes in:\n",
    "- 📸 **Vision**: Analyzing cardboard bundle images\n",
    "- 🧠 **Understanding**: Recognizing flat vs warped bundles  \n",
    "- 💬 **Communication**: Providing clear quality assessments\n",
    "- ⚡ **Efficiency**: Fast inference for production use\n",
    "\n",
    "**Congratulations on successfully fine-tuning your cardboard QC model! 🎊**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}