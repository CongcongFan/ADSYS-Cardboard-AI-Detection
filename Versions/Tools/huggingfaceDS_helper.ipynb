{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd2d6762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 168 CSV rows; usable overlays: 168; copied: 168\n",
      "Wrote: out\\datasets\\qwen_cardboard_qc\\train.jsonl and out\\datasets\\qwen_cardboard_qc\\val.jsonl\n",
      "Train size: 143  |  Val size: 25\n",
      "Sample JSONL line:\n",
      " {\"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"You are an expert cardboard quality inspector. Analyze the cardboard pieces within the green bounding boxes in this image.\\n\\nFocus on:\\n1. WARP: Look for major bending, curving, or deformation from a flat surface, slight bending i ...\n"
     ]
    }
   ],
   "source": [
    "# --- Notebook Cell: Build the JSONL dataset for Qwen2.5-VL via Unsloth ---\n",
    "\n",
    "import os, json, random, shutil\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "# -------------------------------\n",
    "# Configuration (edit if needed)\n",
    "# -------------------------------\n",
    "SRC_OVERLAYS_DIR = Path(\"./out/overlays_overlay\")   # your green-box overlays\n",
    "QC_CSV           = Path(\"./out/qc_labels.csv\")      # file,label,reason\n",
    "COCO_JSON        = Path(\"./train/_annotations.coco.json\")  # optional consistency check\n",
    "\n",
    "OUTPUT_DIR       = Path(\"./out/datasets/qwen_cardboard_qc\")\n",
    "IMAGES_OUT_DIR   = OUTPUT_DIR / \"images\"\n",
    "TRAIN_JSONL      = OUTPUT_DIR / \"train.jsonl\"\n",
    "VAL_JSONL        = OUTPUT_DIR / \"val.jsonl\"\n",
    "SPLIT_VAL_FRAC   = 0.15\n",
    "RANDOM_SEED      = 42\n",
    "\n",
    "# Optional: normalize long side for all images (recommended 300â€“1000 px).\n",
    "# Set to an int like 768 to enable, or None to skip.\n",
    "NORMALIZE_LONG_SIDE = None  # e.g., 768\n",
    "\n",
    "# Your task prompt (kept verbatim, as provided)\n",
    "PROMPT = (\n",
    "    \"You are an expert cardboard quality inspector. Analyze the cardboard pieces within the green bounding boxes in this image.\\n\\n\"\n",
    "    \"Focus on:\\n\"\n",
    "    \"1. WARP: Look for major bending, curving, or deformation from a flat surface, slight bending is acceptable.\\n\"\n",
    "    \" 2. Reason: A short description. \\n\"\n",
    "    \"Return ONLY this exact JSON format with no additional text:\\n\"\n",
    "    \"{\\\"warp\\\": true, \\\"reason\\\": \\\"good\\\"}\\n\\n\"\n",
    "    \"warp: true if ANY major bendingm/curving, false if completely flat or slight bending\"\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Helpers\n",
    "# -------------------------------\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def resize_to_long_side(img: Image.Image, long_side: int) -> Image.Image:\n",
    "    w, h = img.size\n",
    "    if max(w, h) == long_side:\n",
    "        return img\n",
    "    if w >= h:\n",
    "        new_w = long_side\n",
    "        new_h = int(h * (long_side / float(w)))\n",
    "    else:\n",
    "        new_h = long_side\n",
    "        new_w = int(w * (long_side / float(h)))\n",
    "    return img.resize((max(1, new_w), max(1, new_h)), Image.BICUBIC)\n",
    "\n",
    "def stratified_split_indices(indices_by_class, val_frac, rng):\n",
    "    val_indices = set()\n",
    "    for cls, idxs in indices_by_class.items():\n",
    "        idxs = idxs[:]  # copy\n",
    "        rng.shuffle(idxs)\n",
    "        n_val = max(1, int(len(idxs) * val_frac)) if len(idxs) > 0 else 0\n",
    "        val_indices.update(idxs[:n_val])\n",
    "    return val_indices\n",
    "\n",
    "# -------------------------------\n",
    "# Load inputs\n",
    "# -------------------------------\n",
    "assert SRC_OVERLAYS_DIR.exists(), f\"Overlays dir not found: {SRC_OVERLAYS_DIR}\"\n",
    "assert QC_CSV.exists(), f\"Labels CSV not found: {QC_CSV}\"\n",
    "\n",
    "df = pd.read_csv(QC_CSV)\n",
    "expected_cols = {\"file\", \"label\", \"reason\"}\n",
    "missing_cols = expected_cols - set(df.columns.str.lower())\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"qc_labels.csv is missing columns: {missing_cols}. Expect columns: file,label,reason\")\n",
    "\n",
    "# Normalize column names\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "# Optional: COCO check (best-effort, no failure if missing)\n",
    "coco_image_set = set()\n",
    "if COCO_JSON.exists():\n",
    "    try:\n",
    "        with open(COCO_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "            coco = json.load(f)\n",
    "        for im in coco.get(\"images\", []):\n",
    "            coco_image_set.add(im.get(\"file_name\"))\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Could not read COCO file: {e}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Prepare output dirs\n",
    "# -------------------------------\n",
    "ensure_dir(OUTPUT_DIR)\n",
    "ensure_dir(IMAGES_OUT_DIR)\n",
    "\n",
    "# -------------------------------\n",
    "# Build examples\n",
    "# -------------------------------\n",
    "records = []\n",
    "missing_files = []\n",
    "copied = 0\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    filename = row[\"file\"]\n",
    "    label    = str(row[\"label\"]).strip()\n",
    "    reason   = str(row[\"reason\"]).strip()\n",
    "\n",
    "    src_path = SRC_OVERLAYS_DIR / filename\n",
    "    if not src_path.exists():\n",
    "        missing_files.append(filename)\n",
    "        continue\n",
    "\n",
    "    # Target (assistant) mapping:\n",
    "    # Pass  -> warp = False\n",
    "    # Fail  -> warp = True\n",
    "    warp = (label.lower() != \"pass\")\n",
    "    assistant_obj = {\"warp\": bool(warp), \"reason\": reason}\n",
    "    assistant_text = json.dumps(assistant_obj, ensure_ascii=False)  # ensures true/false lowercase\n",
    "\n",
    "    # Copy (and optional resize) to dataset images folder using the same basename\n",
    "    dst_path = IMAGES_OUT_DIR / filename\n",
    "    if NORMALIZE_LONG_SIDE is not None:\n",
    "        try:\n",
    "            with Image.open(src_path) as im:\n",
    "                im = im.convert(\"RGB\")\n",
    "                im = resize_to_long_side(im, NORMALIZE_LONG_SIDE)\n",
    "                im.save(dst_path, quality=95)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Could not resize {src_path}: {e}. Copying as-is.\")\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "    else:\n",
    "        if not dst_path.exists():\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "    copied += 1\n",
    "\n",
    "    # Messages use the repo-relative path \"images/<filename>\"\n",
    "    rel_image = f\"images/{filename}\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\",  \"text\": PROMPT},\n",
    "                {\"type\": \"image\", \"image\": rel_image}\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": assistant_text}\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    records.append({\n",
    "        \"id\": Path(filename).stem,\n",
    "        \"messages\": messages,\n",
    "        # optional metadata for traceability:\n",
    "        \"label\": label,\n",
    "        \"reason\": reason,\n",
    "        \"image\": rel_image\n",
    "    })\n",
    "\n",
    "print(f\"Loaded {len(df)} CSV rows; usable overlays: {len(records)}; copied: {copied}\")\n",
    "if missing_files:\n",
    "    print(f\"[INFO] {len(missing_files)} filenames in CSV missing in overlays:\\n  - \" + \"\\n  - \".join(missing_files[:10]) + (\"\\n  ...\" if len(missing_files) > 10 else \"\"))\n",
    "\n",
    "if coco_image_set:\n",
    "    not_in_coco = [r[\"id\"] for r in records if (Path(r[\"image\"]).name not in coco_image_set)]\n",
    "    if not_in_coco:\n",
    "        print(f\"[INFO] {len(not_in_coco)} overlays not listed in COCO. That's OK for finetune, just FYI.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Stratified split & write JSONL\n",
    "# -------------------------------\n",
    "rng = random.Random(RANDOM_SEED)\n",
    "indices_by_class = {False: [], True: []}\n",
    "for idx, r in enumerate(records):\n",
    "    warp = json.loads(r[\"messages\"][1][\"content\"][0][\"text\"])[\"warp\"]\n",
    "    indices_by_class[warp].append(idx)\n",
    "\n",
    "val_indices = stratified_split_indices(indices_by_class, SPLIT_VAL_FRAC, rng)\n",
    "\n",
    "def write_jsonl(path: Path, idx_set):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, r in enumerate(records):\n",
    "            if (i in idx_set) == True:\n",
    "                f.write(json.dumps({\"messages\": r[\"messages\"]}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def write_jsonl_complement(path: Path, idx_set):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, r in enumerate(records):\n",
    "            if (i in idx_set) == False:\n",
    "                f.write(json.dumps({\"messages\": r[\"messages\"]}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "write_jsonl(VAL_JSONL, val_indices)\n",
    "write_jsonl_complement(TRAIN_JSONL, val_indices)\n",
    "\n",
    "print(f\"Wrote: {TRAIN_JSONL} and {VAL_JSONL}\")\n",
    "print(f\"Train size: {sum(1 for _ in open(TRAIN_JSONL, 'r', encoding='utf-8'))}  |  Val size: {sum(1 for _ in open(VAL_JSONL, 'r', encoding='utf-8'))}\")\n",
    "\n",
    "# Quick peek at one line\n",
    "with open(TRAIN_JSONL, \"r\", encoding=\"utf-8\") as f:\n",
    "    print(\"Sample JSONL line:\\n\", f.readline().strip()[:300] + \" ...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfcba4a",
   "metadata": {},
   "source": [
    "## Push to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f02144",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# --- Notebook Cell: Push JSONL + images to Hugging Face Datasets repo ---\n",
    "\n",
    "# pip installs in-notebook if needed:\n",
    "# %pip install -qU huggingface_hub\n",
    "\n",
    "from huggingface_hub import HfApi, create_repo, upload_folder, upload_file, whoami\n",
    "\n",
    "HF_TOKEN      = \"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\"     # <-- paste your token\n",
    "HF_USERNAME   = None                      # set to None to auto-detect from token\n",
    "DATASET_NAME  = \"cardboard-qc-qwen-vl\"    # you can rename\n",
    "\n",
    "api = HfApi(token=HF_TOKEN)\n",
    "if HF_USERNAME is None:\n",
    "    HF_USERNAME = whoami(token=HF_TOKEN)[\"name\"]\n",
    "\n",
    "repo_id = f\"{HF_USERNAME}/{DATASET_NAME}\"\n",
    "create_repo(repo_id, repo_type=\"dataset\", private=True, exist_ok=True, token=HF_TOKEN)\n",
    "\n",
    "# Upload images folder\n",
    "upload_folder(\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"dataset\",\n",
    "    folder_path=str(IMAGES_OUT_DIR),\n",
    "    path_in_repo=\"images\",\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "# Upload JSONL files\n",
    "upload_file(\n",
    "    path_or_fileobj=str(TRAIN_JSONL),\n",
    "    path_in_repo=\"train.jsonl\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"dataset\",\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "upload_file(\n",
    "    path_or_fileobj=str(VAL_JSONL),\n",
    "    path_in_repo=\"val.jsonl\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"dataset\",\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "# Minimal dataset card with usage snippet\n",
    "readme_text = \"\"\"---\n",
    "pretty_name: Cardboard QC (Qwen2.5-VL)\n",
    "task_categories:\n",
    "- visual-question-answering\n",
    "- image-classification\n",
    "license: cc-by-4.0\n",
    "---\n",
    "\n",
    "# Cardboard QC â€” Qwen2.5-VL\n",
    "\n",
    "Each row is a chat with **messages** following Unsloth's vision finetune format:\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"messages\": [\n",
    "    {{\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {{\"type\": \"text\", \"text\": \"INSTRUCTION...\"}},\n",
    "        {{\"type\": \"image\", \"image\": \"images/IMG_....jpg\"}}\n",
    "      ]\n",
    "    }},\n",
    "    {{\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": [{{\"type\": \"text\", \"text\": \"{{\\\\\"warp\\\\\": true/false, \\\\\"reason\\\\\": \\\\\"...\\\\}}\"}}]\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "Example load\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"{repo_id}\")\n",
    "print(ds[\"train\"][0][\"messages\"][0][\"content\"])\n",
    "\"\"\"\n",
    "Images are under images/. The user instruction asks the model to focus on green bounding boxes drawn in the overlays.\n",
    "\n",
    "readme_path = OUTPUT_DIR / \"README.md\"\n",
    "with open(readme_path, \"w\", encoding=\"utf-8\") as f:\n",
    "f.write(readme_text)\n",
    "\n",
    "upload_file(\n",
    "path_or_fileobj=str(readme_path),\n",
    "path_in_repo=\"README.md\",\n",
    "repo_id=repo_id,\n",
    "repo_type=\"dataset\",\n",
    "token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "print(f\"âœ… Uploaded dataset to https://huggingface.co/datasets/{repo_id}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8d9542",
   "metadata": {},
   "source": [
    "## Play with Chat temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b205262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Unsloth currently only works on NVIDIA GPUs and Intel GPUs.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(torch.cuda.is_available())\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_templates\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CHAT_TEMPLATES\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlist\u001b[39m(CHAT_TEMPLATES.keys()))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\unsloth\\__init__.py:79\u001b[39m\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnsloth currently only works on NVIDIA GPUs and Intel GPUs.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m DEVICE_TYPE : \u001b[38;5;28mstr\u001b[39m = \u001b[43mget_device_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_device_count\u001b[39m():\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m DEVICE_TYPE == \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\unsloth\\__init__.py:77\u001b[39m, in \u001b[36mget_device_type\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch, \u001b[33m\"\u001b[39m\u001b[33mxpu\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m torch.xpu.is_available():\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mxpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnsloth currently only works on NVIDIA GPUs and Intel GPUs.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNotImplementedError\u001b[39m: Unsloth currently only works on NVIDIA GPUs and Intel GPUs."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "from unsloth.chat_templates import CHAT_TEMPLATES\n",
    "print(list(CHAT_TEMPLATES.keys()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
